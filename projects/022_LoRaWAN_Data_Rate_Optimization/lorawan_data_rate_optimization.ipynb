{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47c2c7e",
   "metadata": {},
   "source": [
    "# Project 22: Optimizing LoRaWAN Data Rate using Reinforcement Learning\n",
    "\n",
    "**Objective:** Train a Reinforcement Learning agent that can dynamically select the optimal Spreading Factor (SF) for a LoRaWAN end-device to maximize successful transmission probability while minimizing energy consumption.\n",
    "\n",
    "**Environment:** Simulated LoRaWAN Channel with physics-based modeling\n",
    "\n",
    "**Model:** Q-Learning - foundational RL algorithm perfect for learning optimal actions in given states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-libraries",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environment-setup",
   "metadata": {},
   "source": [
    "## 2. Build the Simulated LoRaWAN Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lorawan-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Building the Simulated LoRaWAN Environment ---\")\n",
    "\n",
    "class LoRaWANEnv:\n",
    "    def __init__(self):\n",
    "        # Actions: 6 possible Spreading Factors (SF7 to SF12)\n",
    "        self.actions = [7, 8, 9, 10, 11, 12]\n",
    "        # Required SNR (in dB) for a successful transmission at each SF\n",
    "        self.snr_thresholds = {7: -7.5, 8: -10, 9: -12.5, 10: -15, 11: -17.5, 12: -20}\n",
    "        # Relative time on air (energy cost) for each SF\n",
    "        self.time_on_air = {7: 1, 8: 1.8, 9: 3.2, 10: 5.8, 11: 11, 12: 21}\n",
    "        \n",
    "        # States: Discretized SNR values from -25 dB to 0 dB in steps of 2.5 dB\n",
    "        self.states = np.arange(-25, 2.5, 2.5)\n",
    "        self.state_space_size = len(self.states)\n",
    "        self.action_space_size = len(self.actions)\n",
    "        \n",
    "    def get_state_index(self, snr):\n",
    "        # Find the closest discretized state for a given continuous SNR value\n",
    "        return np.abs(self.states - snr).argmin()\n",
    "\n",
    "    def step(self, state_idx, action_idx):\n",
    "        current_snr = self.states[state_idx]\n",
    "        chosen_sf = self.actions[action_idx]\n",
    "        \n",
    "        # --- Environment Physics ---\n",
    "        # Check if the transmission is successful\n",
    "        if current_snr >= self.snr_thresholds[chosen_sf]:\n",
    "            success = True\n",
    "            reward = 100  # Large reward for success\n",
    "        else:\n",
    "            success = False\n",
    "            reward = -200 # Large penalty for failure\n",
    "        \n",
    "        # Add a penalty proportional to the energy used (time on air)\n",
    "        reward -= self.time_on_air[chosen_sf]\n",
    "        \n",
    "        # Simulate the next state (e.g., SNR changes slightly due to environmental factors)\n",
    "        next_snr = current_snr + np.random.normal(0, 1.0)\n",
    "        next_snr = np.clip(next_snr, -25, 0) # Keep SNR within bounds\n",
    "        next_state_idx = self.get_state_index(next_snr)\n",
    "        \n",
    "        return next_state_idx, reward, success\n",
    "\n",
    "# Instantiate the environment\n",
    "env = LoRaWANEnv()\n",
    "print(\"Environment built successfully.\")\n",
    "print(f\"State space size: {env.state_space_size}\")\n",
    "print(f\"Action space size: {env.action_space_size}\")\n",
    "print(f\"SNR thresholds: {env.snr_thresholds}\")\n",
    "print(f\"Time on air costs: {env.time_on_air}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q-learning-training",
   "metadata": {},
   "source": [
    "## 3. Q-Learning Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Training the Q-Learning Agent ---\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_episodes = 20000\n",
    "alpha = 0.1      # Learning rate\n",
    "gamma = 0.9      # Discount factor\n",
    "epsilon = 1.0    # Exploration rate\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.0005\n",
    "\n",
    "# Initialize Q-table with zeros (rows=states, columns=actions)\n",
    "q_table = np.zeros((env.state_space_size, env.action_space_size))\n",
    "rewards_per_episode = []\n",
    "\n",
    "print(f\"Training for {num_episodes} episodes...\")\n",
    "print(f\"Q-table shape: {q_table.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    # Start with a random SNR state\n",
    "    state = random.randint(0, env.state_space_size - 1)\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Epsilon-greedy action selection\n",
    "    if random.uniform(0, 1) > epsilon:\n",
    "        action = np.argmax(q_table[state, :]) # Exploit\n",
    "    else:\n",
    "        action = random.randint(0, env.action_space_size - 1) # Explore\n",
    "        \n",
    "    next_state, reward, _ = env.step(state, action)\n",
    "    \n",
    "    # Q-Learning update rule\n",
    "    q_table[state, action] = q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state, :]) - q_table[state, action])\n",
    "    \n",
    "    total_reward += reward\n",
    "    rewards_per_episode.append(total_reward)\n",
    "    \n",
    "    # Update epsilon (exploration-exploitation trade-off)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "    \n",
    "    # Progress reporting\n",
    "    if (episode + 1) % 5000 == 0:\n",
    "        avg_reward = np.mean(rewards_per_episode[-1000:])\n",
    "        print(f\"Episode {episode + 1}: Average reward (last 1000): {avg_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "policy-analysis",
   "metadata": {},
   "source": [
    "## 4. Analysis and Visualization of Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Analyzing the Learned Policy ---\")\n",
    "\n",
    "# Extract the optimal policy from the Q-table\n",
    "# For each state (SNR level), the best action is the one with the highest Q-value.\n",
    "optimal_policy = np.argmax(q_table, axis=1)\n",
    "policy_df = pd.DataFrame({\n",
    "    'SNR (dB)': env.states,\n",
    "    'Optimal SF': [env.actions[p] for p in optimal_policy]\n",
    "})\n",
    "\n",
    "print(\"Learned Optimal Policy:\")\n",
    "print(policy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-qtable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize the Q-table ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(q_table, cmap='viridis', xticklabels=env.actions, yticklabels=np.round(env.states, 1))\n",
    "plt.title('Learned Q-Table Values', fontsize=16)\n",
    "plt.xlabel('Action (Spreading Factor)')\n",
    "plt.ylabel('State (Signal-to-Noise Ratio)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize the Learned Policy ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(policy_df['SNR (dB)'], policy_df['Optimal SF'], marker='o', linestyle='--')\n",
    "plt.title('Optimal LoRaWAN Spreading Factor vs. SNR', fontsize=16)\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Optimal SF')\n",
    "plt.grid(True)\n",
    "plt.gca().invert_xaxis() # Better visualization with high SNR on the left\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize Learning Progress ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Calculate a moving average of rewards to see the trend\n",
    "moving_avg = pd.Series(rewards_per_episode).rolling(window=500).mean()\n",
    "plt.plot(moving_avg)\n",
    "plt.title('Agent Learning Progress (Moving Average of Reward per Episode)', fontsize=16)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deployment-function",
   "metadata": {},
   "source": [
    "## 5. Real-World Deployment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deployment-function-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_optimal_sf(current_snr, q_table, env):\n",
    "    \"\"\"\n",
    "    Select optimal Spreading Factor for current channel conditions\n",
    "    \n",
    "    Args:\n",
    "        current_snr: Current signal-to-noise ratio in dB\n",
    "        q_table: Trained Q-table\n",
    "        env: LoRaWAN environment\n",
    "    \n",
    "    Returns:\n",
    "        Optimal spreading factor (SF7-SF12)\n",
    "    \"\"\"\n",
    "    state_idx = env.get_state_index(current_snr)\n",
    "    action_idx = np.argmax(q_table[state_idx, :])\n",
    "    return env.actions[action_idx]\n",
    "\n",
    "# Test the deployment function with example SNR values\n",
    "test_snrs = [-5, -10, -15, -20, -25]\n",
    "print(\"\\n--- Testing Deployment Function ---\")\n",
    "print(\"SNR (dB) -> Optimal SF\")\n",
    "print(\"----------------------\")\n",
    "for snr in test_snrs:\n",
    "    optimal_sf = select_optimal_sf(snr, q_table, env)\n",
    "    print(f\"{snr:6} -> SF{optimal_sf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-analysis",
   "metadata": {},
   "source": [
    "## 6. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze final policy performance\n",
    "print(\"\\n--- Performance Analysis ---\")\n",
    "\n",
    "# Calculate success rate and average energy consumption for the learned policy\n",
    "num_test_episodes = 1000\n",
    "successes = 0\n",
    "total_energy = 0\n",
    "\n",
    "for _ in range(num_test_episodes):\n",
    "    # Random initial state\n",
    "    state = random.randint(0, env.state_space_size - 1)\n",
    "    \n",
    "    # Use learned policy (no exploration)\n",
    "    action = np.argmax(q_table[state, :])\n",
    "    \n",
    "    # Execute action\n",
    "    _, reward, success = env.step(state, action)\n",
    "    \n",
    "    if success:\n",
    "        successes += 1\n",
    "    \n",
    "    total_energy += env.time_on_air[env.actions[action]]\n",
    "\n",
    "success_rate = successes / num_test_episodes\n",
    "avg_energy = total_energy / num_test_episodes\n",
    "\n",
    "print(f\"Success Rate: {success_rate:.2%}\")\n",
    "print(f\"Average Energy Consumption: {avg_energy:.2f} time units\")\n",
    "print(f\"Final Training Reward (last 100 episodes): {np.mean(rewards_per_episode[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conclusion-notes",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Conclusion ---\")\n",
    "print(\"The Q-Learning agent successfully learned an intelligent policy for selecting the LoRaWAN Spreading Factor.\")\n",
    "print(\"Key Takeaways:\")\n",
    "print(\"- The final policy is highly logical and reflects networking best practices: When the signal is strong (high SNR), the agent chooses a low SF (like SF7) for fast, energy-efficient communication. When the signal is weak (low SNR), it correctly switches to a high SF (like SF12) for a more robust, long-range connection.\")\n",
    "print(\"- The Q-table heatmap visually confirms this logic, showing high Q-values for low SFs at high SNRs and high Q-values for high SFs at low SNRs.\")\n",
    "print(\"- The learning progress chart shows that the agent's performance steadily improved over time, demonstrating that it was effectively learning from its successes and failures.\")\n",
    "print(\"- This is a powerful demonstration of how Reinforcement Learning can be used to create truly adaptive and autonomous network protocols that optimize their own performance in response to changing environmental conditions, without needing to be explicitly programmed with complex 'if-then-else' rules.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}