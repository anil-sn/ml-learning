{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "project-header",
   "metadata": {},
   "source": [
    "# Project 34: Detecting Noisy Neighbors in a Multi-tenant Cloud Environment\n",
    "\n",
    "**Objective:** Build an unsupervised anomaly detection model that can identify a \"noisy neighbor\" (a high-resource-consuming tenant) on a shared host by analyzing the network traffic patterns of all tenants and flagging outliers.\n",
    "\n",
    "**Dataset Source:** Synthetically Generated (simulated multi-tenant network traffic with periodic noisy neighbor behavior)\n",
    "\n",
    "**Model:** Isolation Forest for unsupervised outlier detection\n",
    "\n",
    "**Instructions:**\n",
    "This notebook is fully self-contained and does not require external files. Simply run all cells in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Project 34: Noisy Neighbors Detection - Setup and Imports\n",
    "# ==================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Synthetic Tenant Traffic Data Generation\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Generating Synthetic Multi-Tenant Network Traffic Dataset ---\")\n",
    "\n",
    "# Simulation parameters\n",
    "num_tenants = 20\n",
    "time_steps = 1000  # Simulate 1000 time intervals (e.g., minutes)\n",
    "data = []\n",
    "tenants = [f'tenant_{i+1}' for i in range(num_tenants)]\n",
    "noisy_neighbor_tenant = 'tenant_5'\n",
    "secondary_noisy_tenant = 'tenant_15'  # Add a second potential noisy neighbor\n",
    "\n",
    "print(f\"Simulation parameters:\")\n",
    "print(f\"• Number of tenants: {num_tenants}\")\n",
    "print(f\"• Time steps: {time_steps}\")\n",
    "print(f\"• Primary noisy neighbor: {noisy_neighbor_tenant}\")\n",
    "print(f\"• Secondary noisy neighbor: {secondary_noisy_tenant}\")\n",
    "\n",
    "for t in range(time_steps):\n",
    "    for tenant in tenants:\n",
    "        is_noisy = False\n",
    "        \n",
    "        # Define normal behavior with some variation based on tenant\n",
    "        if 'tenant_1' in tenant or 'tenant_2' in tenant:  # Low activity tenants\n",
    "            base_pps = max(0, np.random.normal(2000, 500))  # Lower baseline\n",
    "            base_bps = base_pps * np.random.normal(250, 30)\n",
    "        elif 'tenant_19' in tenant or 'tenant_20' in tenant:  # High activity tenants\n",
    "            base_pps = max(0, np.random.normal(8000, 1200))  # Higher baseline\n",
    "            base_bps = base_pps * np.random.normal(400, 60)\n",
    "        else:  # Normal tenants\n",
    "            base_pps = max(0, np.random.normal(5000, 1000))  # Packets per second\n",
    "            base_bps = base_pps * np.random.normal(300, 50)  # Bytes per second\n",
    "        \n",
    "        # Add time-based patterns (some tenants are more active during certain periods)\n",
    "        time_factor = 1 + 0.3 * np.sin(2 * np.pi * t / 100)  # Daily pattern\n",
    "        base_pps *= time_factor\n",
    "        base_bps *= time_factor\n",
    "        \n",
    "        # --- Simulate the Primary Noisy Neighbor event ---\n",
    "        # The noisy neighbor has a burst of high activity for a specific period\n",
    "        if tenant == noisy_neighbor_tenant and 400 <= t < 600:\n",
    "            base_pps *= np.random.uniform(5, 10)  # 5-10x more packets\n",
    "            base_bps *= np.random.uniform(5, 10)  # 5-10x more bytes\n",
    "            is_noisy = True\n",
    "        \n",
    "        # --- Simulate a Secondary Noisy Neighbor event ---\n",
    "        # Shorter, more intense burst from another tenant\n",
    "        if tenant == secondary_noisy_tenant and 750 <= t < 800:\n",
    "            base_pps *= np.random.uniform(8, 15)  # Even higher spike\n",
    "            base_bps *= np.random.uniform(8, 15)\n",
    "            is_noisy = True\n",
    "        \n",
    "        # Add some random spikes for other tenants (false positives to test robustness)\n",
    "        if not is_noisy and np.random.random() < 0.002:  # 0.2% chance of random spike\n",
    "            base_pps *= np.random.uniform(2, 4)\n",
    "            base_bps *= np.random.uniform(2, 4)\n",
    "        \n",
    "        # Calculate additional derived metrics\n",
    "        avg_packet_size = base_bps / max(base_pps, 1)  # Avoid division by zero\n",
    "        network_utilization = min(base_bps / 1000000, 100)  # Percentage of 1Mbps link\n",
    "        \n",
    "        data.append([t, tenant, base_pps, base_bps, avg_packet_size, \n",
    "                    network_utilization, is_noisy])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['timestamp', 'tenant_id', 'packets_per_second', \n",
    "                                'bytes_per_second', 'avg_packet_size', \n",
    "                                'network_utilization', 'is_truly_noisy'])\n",
    "\n",
    "# Ensure no negative values\n",
    "df['packets_per_second'] = df['packets_per_second'].clip(lower=0)\n",
    "df['bytes_per_second'] = df['bytes_per_second'].clip(lower=0)\n",
    "\n",
    "print(f\"\\nDataset generation complete. Created {len(df)} records.\")\n",
    "print(f\"Total noisy neighbor events: {df['is_truly_noisy'].sum()}\")\n",
    "print(f\"Percentage of noisy events: {(df['is_truly_noisy'].sum() / len(df)) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nDataset Sample:\")\n",
    "print(df.sample(10).round(2))\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(df.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Data Exploration and Visualization\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Data Exploration and Pattern Analysis ---\")\n",
    "\n",
    "# Create comprehensive exploration visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "fig.suptitle('Multi-Tenant Network Traffic Analysis', fontsize=16)\n",
    "\n",
    "# 1. Time series of packets per second for all tenants\n",
    "pivot_data = df.pivot(index='timestamp', columns='tenant_id', values='packets_per_second')\n",
    "ax1.plot(pivot_data.index, pivot_data[noisy_neighbor_tenant], \n",
    "         color='red', linewidth=2, label=f'{noisy_neighbor_tenant} (Primary Noisy)')\n",
    "ax1.plot(pivot_data.index, pivot_data[secondary_noisy_tenant], \n",
    "         color='orange', linewidth=2, label=f'{secondary_noisy_tenant} (Secondary Noisy)')\n",
    "\n",
    "# Plot a few normal tenants\n",
    "normal_tenants = ['tenant_1', 'tenant_3', 'tenant_10']\n",
    "for tenant in normal_tenants:\n",
    "    ax1.plot(pivot_data.index, pivot_data[tenant], alpha=0.7, linewidth=1)\n",
    "\n",
    "ax1.set_title('Packets per Second Over Time')\n",
    "ax1.set_xlabel('Time Step')\n",
    "ax1.set_ylabel('Packets per Second')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Distribution of network metrics\n",
    "normal_data = df[df['is_truly_noisy'] == False]\n",
    "noisy_data = df[df['is_truly_noisy'] == True]\n",
    "\n",
    "ax2.hist(normal_data['packets_per_second'], bins=50, alpha=0.7, \n",
    "         label='Normal', color='blue', density=True)\n",
    "ax2.hist(noisy_data['packets_per_second'], bins=30, alpha=0.7, \n",
    "         label='Noisy Neighbor', color='red', density=True)\n",
    "ax2.set_title('Distribution of Packets per Second')\n",
    "ax2.set_xlabel('Packets per Second')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Scatter plot of packets vs bytes\n",
    "ax3.scatter(normal_data['packets_per_second'], normal_data['bytes_per_second'], \n",
    "           alpha=0.6, label='Normal', s=20, color='blue')\n",
    "ax3.scatter(noisy_data['packets_per_second'], noisy_data['bytes_per_second'], \n",
    "           alpha=0.8, label='Noisy Neighbor', s=30, color='red')\n",
    "ax3.set_title('Packets vs Bytes per Second')\n",
    "ax3.set_xlabel('Packets per Second')\n",
    "ax3.set_ylabel('Bytes per Second')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Tenant activity heatmap\n",
    "tenant_stats = df.groupby('tenant_id').agg({\n",
    "    'packets_per_second': ['mean', 'max', 'std'],\n",
    "    'is_truly_noisy': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "tenant_stats.columns = ['_'.join(col).strip() for col in tenant_stats.columns]\n",
    "tenant_means = tenant_stats['packets_per_second_mean'].values.reshape(4, 5)  # 4x5 grid\n",
    "\n",
    "im = ax4.imshow(tenant_means, cmap='YlOrRd', aspect='auto')\n",
    "ax4.set_title('Average Packets per Second by Tenant\\n(Arranged in 4x5 Grid)')\n",
    "ax4.set_xticks(range(5))\n",
    "ax4.set_yticks(range(4))\n",
    "ax4.set_xlabel('Tenant Column')\n",
    "ax4.set_ylabel('Tenant Row')\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(im, ax=ax4, label='Avg Packets/sec')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display tenant behavior summary\n",
    "print(\"\\nTenant Behavior Summary:\")\n",
    "tenant_summary = df.groupby('tenant_id').agg({\n",
    "    'packets_per_second': ['mean', 'max', 'std'],\n",
    "    'bytes_per_second': ['mean', 'max'],\n",
    "    'is_truly_noisy': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "tenant_summary.columns = ['_'.join(col).strip() for col in tenant_summary.columns]\n",
    "tenant_summary = tenant_summary.sort_values('is_truly_noisy_sum', ascending=False)\n",
    "print(tenant_summary.head(10))\n",
    "\n",
    "print(f\"\\nNoisy neighbor events by tenant:\")\n",
    "noisy_events = df[df['is_truly_noisy'] == True]['tenant_id'].value_counts()\n",
    "print(noisy_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Data Preparation and Feature Engineering\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Preparing Data for Unsupervised Learning ---\")\n",
    "\n",
    "# The features we'll use to detect anomalies\n",
    "feature_cols = ['packets_per_second', 'bytes_per_second', 'avg_packet_size', 'network_utilization']\n",
    "X = df[feature_cols].copy()\n",
    "\n",
    "print(f\"Original feature matrix shape: {X.shape}\")\n",
    "print(f\"Features used for anomaly detection: {feature_cols}\")\n",
    "\n",
    "# Handle any potential NaN or infinite values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "print(f\"\\nFeature statistics before scaling:\")\n",
    "print(X.describe().round(2))\n",
    "\n",
    "# Scale the features for better model performance\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\nFeatures scaled using StandardScaler\")\n",
    "print(f\"Scaled feature matrix shape: {X_scaled.shape}\")\n",
    "\n",
    "# Display scaling parameters\n",
    "print(f\"\\nScaling parameters:\")\n",
    "for i, feature in enumerate(feature_cols):\n",
    "    print(f\"  {feature}: mean={scaler.mean_[i]:.2f}, std={scaler.scale_[i]:.2f}\")\n",
    "\n",
    "# Store labels for evaluation (though we won't use them for training)\n",
    "y_true = df['is_truly_noisy'].values\n",
    "print(f\"\\nGround truth labels available for evaluation: {len(y_true)} samples\")\n",
    "print(f\"True noisy neighbor samples: {np.sum(y_true)} ({np.mean(y_true)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Model Training (Unsupervised)\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Model Training ---\")\n",
    "\n",
    "# Calculate expected contamination based on our data\n",
    "expected_contamination = np.mean(y_true)\n",
    "print(f\"Expected contamination based on ground truth: {expected_contamination:.4f} ({expected_contamination*100:.2f}%)\")\n",
    "\n",
    "# The `contamination` parameter is the expected proportion of outliers in the data.\n",
    "# We know our noisy events: Primary (200 events) + Secondary (50 events) = 250 events\n",
    "# Out of total: 1000 time_steps * 20 tenants = 20,000 events\n",
    "# So, the approximate contamination is 250 / 20,000 = 0.0125 or 1.25%.\n",
    "contamination_rate = 0.015  # Slightly higher to be conservative\n",
    "\n",
    "model = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=contamination_rate,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nIsolation Forest Configuration:\")\n",
    "print(f\"• Number of estimators: {model.n_estimators}\")\n",
    "print(f\"• Contamination rate: {model.contamination} ({model.contamination*100:.1f}%)\")\n",
    "print(f\"• Random state: {model.random_state}\")\n",
    "\n",
    "print(\"\\nTraining the Isolation Forest model on the entire dataset...\")\n",
    "start_time = time.time()\n",
    "model.fit(X_scaled)\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# Make predictions (1 = normal, -1 = outlier/anomaly)\n",
    "y_pred_raw = model.predict(X_scaled)\n",
    "# Convert to binary (0 = normal, 1 = anomaly) for consistency with ground truth\n",
    "y_pred = (y_pred_raw == -1).astype(int)\n",
    "\n",
    "# Calculate anomaly scores\n",
    "anomaly_scores = model.decision_function(X_scaled)\n",
    "\n",
    "print(f\"\\nPrediction Summary:\")\n",
    "print(f\"• Total samples: {len(y_pred)}\")\n",
    "print(f\"• Predicted anomalies: {np.sum(y_pred)} ({np.mean(y_pred)*100:.2f}%)\")\n",
    "print(f\"• Actual anomalies: {np.sum(y_true)} ({np.mean(y_true)*100:.2f}%)\")\n",
    "print(f\"• Anomaly score range: [{np.min(anomaly_scores):.3f}, {np.max(anomaly_scores):.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Model Evaluation\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Model Evaluation ---\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average='binary', pos_label=1\n",
    ")\n",
    "\n",
    "print(f\"\\nBinary Classification Metrics:\")\n",
    "print(f\"• Precision: {precision:.3f} (of predicted anomalies, what % were actually anomalies)\")\n",
    "print(f\"• Recall: {recall:.3f} (of actual anomalies, what % were detected)\")\n",
    "print(f\"• F1-Score: {f1:.3f} (harmonic mean of precision and recall)\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Normal', 'Noisy Neighbor']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"Actual    Normal  Anomaly\")\n",
    "print(f\"Normal    {cm[0,0]:6d}  {cm[0,1]:7d}\")\n",
    "print(f\"Anomaly   {cm[1,0]:6d}  {cm[1,1]:7d}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "print(f\"\\nAdditional Metrics:\")\n",
    "print(f\"• Accuracy: {accuracy:.3f}\")\n",
    "print(f\"• Specificity: {specificity:.3f} (true negative rate)\")\n",
    "print(f\"• False Positive Rate: {false_positive_rate:.3f}\")\n",
    "print(f\"• True Positives: {tp}\")\n",
    "print(f\"• False Positives: {fp}\")\n",
    "print(f\"• True Negatives: {tn}\")\n",
    "print(f\"• False Negatives: {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Results Visualization\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Results Visualization ---\")\n",
    "\n",
    "# Create comprehensive results visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "fig.suptitle('Noisy Neighbor Detection Results', fontsize=16)\n",
    "\n",
    "# 1. Confusion matrix heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Noisy Neighbor'], \n",
    "            yticklabels=['Normal', 'Noisy Neighbor'], ax=ax1)\n",
    "ax1.set_title('Confusion Matrix')\n",
    "ax1.set_ylabel('Actual')\n",
    "ax1.set_xlabel('Predicted')\n",
    "\n",
    "# 2. Anomaly scores distribution\n",
    "normal_scores = anomaly_scores[y_true == 0]\n",
    "anomaly_scores_true = anomaly_scores[y_true == 1]\n",
    "\n",
    "ax2.hist(normal_scores, bins=50, alpha=0.7, label='Normal', color='blue', density=True)\n",
    "ax2.hist(anomaly_scores_true, bins=30, alpha=0.7, label='True Anomalies', color='red', density=True)\n",
    "ax2.axvline(model.offset_, color='green', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "ax2.set_title('Anomaly Scores Distribution')\n",
    "ax2.set_xlabel('Anomaly Score')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Time series with detected anomalies\n",
    "df_viz = df.copy()\n",
    "df_viz['predicted_anomaly'] = y_pred\n",
    "df_viz['anomaly_score'] = anomaly_scores\n",
    "\n",
    "# Focus on the noisy neighbor tenant\n",
    "noisy_tenant_data = df_viz[df_viz['tenant_id'] == noisy_neighbor_tenant]\n",
    "ax3.plot(noisy_tenant_data['timestamp'], noisy_tenant_data['packets_per_second'], \n",
    "         color='blue', alpha=0.7, label='Traffic')\n",
    "\n",
    "# Highlight true anomalies\n",
    "true_anomalies = noisy_tenant_data[noisy_tenant_data['is_truly_noisy'] == 1]\n",
    "ax3.scatter(true_anomalies['timestamp'], true_anomalies['packets_per_second'], \n",
    "           color='red', s=50, label='True Anomalies', zorder=5)\n",
    "\n",
    "# Highlight detected anomalies\n",
    "detected_anomalies = noisy_tenant_data[noisy_tenant_data['predicted_anomaly'] == 1]\n",
    "ax3.scatter(detected_anomalies['timestamp'], detected_anomalies['packets_per_second'], \n",
    "           color='orange', s=30, marker='x', label='Detected Anomalies', zorder=5)\n",
    "\n",
    "ax3.set_title(f'Anomaly Detection Results for {noisy_neighbor_tenant}')\n",
    "ax3.set_xlabel('Time Step')\n",
    "ax3.set_ylabel('Packets per Second')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature space visualization (PCA for 2D visualization)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plot normal and anomalous points\n",
    "normal_mask = y_true == 0\n",
    "anomaly_mask = y_true == 1\n",
    "detected_mask = y_pred == 1\n",
    "\n",
    "ax4.scatter(X_pca[normal_mask, 0], X_pca[normal_mask, 1], \n",
    "           alpha=0.6, s=20, color='blue', label='Normal')\n",
    "ax4.scatter(X_pca[anomaly_mask, 0], X_pca[anomaly_mask, 1], \n",
    "           alpha=0.8, s=60, color='red', label='True Anomalies')\n",
    "ax4.scatter(X_pca[detected_mask, 0], X_pca[detected_mask, 1], \n",
    "           alpha=0.6, s=40, facecolors='none', edgecolors='orange', \n",
    "           linewidth=2, label='Detected Anomalies')\n",
    "\n",
    "ax4.set_title('Feature Space Visualization (PCA)')\n",
    "ax4.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax4.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPCA Analysis:\")\n",
    "print(f\"• PC1 explains {pca.explained_variance_ratio_[0]:.1%} of variance\")\n",
    "print(f\"• PC2 explains {pca.explained_variance_ratio_[1]:.1%} of variance\")\n",
    "print(f\"• Total variance explained: {sum(pca.explained_variance_ratio_):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tenant-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Tenant-wise Analysis\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Tenant-wise Anomaly Detection Analysis ---\")\n",
    "\n",
    "# Add predictions to the dataframe for analysis\n",
    "df_analysis = df.copy()\n",
    "df_analysis['predicted_anomaly'] = y_pred\n",
    "df_analysis['anomaly_score'] = anomaly_scores\n",
    "\n",
    "# Analyze detection performance by tenant\n",
    "tenant_analysis = []\n",
    "for tenant in tenants:\n",
    "    tenant_data = df_analysis[df_analysis['tenant_id'] == tenant]\n",
    "    \n",
    "    total_samples = len(tenant_data)\n",
    "    true_anomalies = tenant_data['is_truly_noisy'].sum()\n",
    "    detected_anomalies = tenant_data['predicted_anomaly'].sum()\n",
    "    \n",
    "    # Calculate metrics for this tenant\n",
    "    if true_anomalies > 0:\n",
    "        true_positives = ((tenant_data['is_truly_noisy'] == 1) & \n",
    "                         (tenant_data['predicted_anomaly'] == 1)).sum()\n",
    "        tenant_recall = true_positives / true_anomalies\n",
    "        tenant_precision = true_positives / detected_anomalies if detected_anomalies > 0 else 0\n",
    "    else:\n",
    "        tenant_recall = 0\n",
    "        tenant_precision = 0 if detected_anomalies == 0 else np.nan  # False positives only\n",
    "    \n",
    "    avg_anomaly_score = tenant_data['anomaly_score'].mean()\n",
    "    min_anomaly_score = tenant_data['anomaly_score'].min()\n",
    "    \n",
    "    tenant_analysis.append({\n",
    "        'tenant_id': tenant,\n",
    "        'total_samples': total_samples,\n",
    "        'true_anomalies': true_anomalies,\n",
    "        'detected_anomalies': detected_anomalies,\n",
    "        'recall': tenant_recall,\n",
    "        'precision': tenant_precision,\n",
    "        'avg_anomaly_score': avg_anomaly_score,\n",
    "        'min_anomaly_score': min_anomaly_score\n",
    "    })\n",
    "\n",
    "tenant_df = pd.DataFrame(tenant_analysis)\n",
    "tenant_df = tenant_df.sort_values('true_anomalies', ascending=False)\n",
    "\n",
    "print(\"\\nTenant Detection Performance:\")\n",
    "print(tenant_df.round(3))\n",
    "\n",
    "# Focus on tenants with actual anomalies\n",
    "noisy_tenants = tenant_df[tenant_df['true_anomalies'] > 0]\n",
    "print(f\"\\nNoisy Tenant Detection Summary:\")\n",
    "for _, row in noisy_tenants.iterrows():\n",
    "    print(f\"• {row['tenant_id']}:\")\n",
    "    print(f\"  - True anomalies: {row['true_anomalies']}\")\n",
    "    print(f\"  - Detected anomalies: {row['detected_anomalies']}\")\n",
    "    print(f\"  - Recall: {row['recall']:.3f}\")\n",
    "    print(f\"  - Precision: {row['precision']:.3f}\")\n",
    "    print(f\"  - Avg anomaly score: {row['avg_anomaly_score']:.3f}\")\n",
    "\n",
    "# Identify false positive cases\n",
    "false_positive_tenants = tenant_df[(tenant_df['true_anomalies'] == 0) & \n",
    "                                  (tenant_df['detected_anomalies'] > 0)]\n",
    "if len(false_positive_tenants) > 0:\n",
    "    print(f\"\\nFalse Positive Analysis:\")\n",
    "    print(f\"Tenants with false positive detections: {len(false_positive_tenants)}\")\n",
    "    for _, row in false_positive_tenants.iterrows():\n",
    "        print(f\"• {row['tenant_id']}: {row['detected_anomalies']} false positives\")\n",
    "\n",
    "# Calculate overall detection statistics\n",
    "total_true_anomalies = tenant_df['true_anomalies'].sum()\n",
    "total_detected_anomalies = tenant_df['detected_anomalies'].sum()\n",
    "successful_detections = noisy_tenants['recall'].sum() * noisy_tenants['true_anomalies'].sum()\n",
    "\n",
    "print(f\"\\nOverall Detection Statistics:\")\n",
    "print(f\"• Total true anomalies across all tenants: {total_true_anomalies}\")\n",
    "print(f\"• Total detected anomalies: {total_detected_anomalies}\")\n",
    "print(f\"• Tenants with noisy behavior detected: {len(noisy_tenants)}\")\n",
    "print(f\"• Average recall for noisy tenants: {noisy_tenants['recall'].mean():.3f}\")\n",
    "print(f\"• Average precision for noisy tenants: {noisy_tenants['precision'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Conclusion\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Conclusion ---\")\n",
    "print(\"The Isolation Forest model successfully learned to detect noisy neighbors in the multi-tenant cloud environment.\")\n",
    "\n",
    "print(\"\\nKey Performance Results:\")\n",
    "print(f\"• Overall accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "print(f\"• Precision: {precision:.3f} (reliability of anomaly alerts)\")\n",
    "print(f\"• Recall: {recall:.3f} (coverage of actual noisy neighbors)\")\n",
    "print(f\"• F1-Score: {f1:.3f} (balanced performance metric)\")\n",
    "print(f\"• False positive rate: {false_positive_rate:.3f} ({false_positive_rate*100:.1f}%)\")\n",
    "print(f\"• Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nDetection Effectiveness:\")\n",
    "print(f\"• Primary noisy neighbor ({noisy_neighbor_tenant}): {noisy_tenants[noisy_tenants['tenant_id'] == noisy_neighbor_tenant]['recall'].iloc[0]:.1%} detection rate\")\n",
    "print(f\"• Secondary noisy neighbor ({secondary_noisy_tenant}): {noisy_tenants[noisy_tenants['tenant_id'] == secondary_noisy_tenant]['recall'].iloc[0]:.1%} detection rate\")\n",
    "print(f\"• Total noisy events detected: {tp} out of {tp + fn}\")\n",
    "print(f\"• False alarms generated: {fp} out of {total_detected_anomalies} alerts\")\n",
    "\n",
    "print(\"\\nBusiness Impact:\")\n",
    "print(\"• **Performance Isolation**: Proactively identify tenants causing performance degradation\")\n",
    "print(\"• **Resource Management**: Enable targeted resource throttling or migration\")\n",
    "print(\"• **SLA Protection**: Prevent noisy neighbors from impacting other tenants' SLAs\")\n",
    "print(\"• **Cost Optimization**: Identify opportunities for workload balancing and right-sizing\")\n",
    "\n",
    "print(\"\\nOperational Applications:\")\n",
    "print(\"• **Real-time Monitoring**: Deploy for continuous tenant behavior analysis\")\n",
    "print(\"• **Automated Alerting**: Generate alerts when anomalous behavior is detected\")\n",
    "print(\"• **Capacity Planning**: Use insights for better resource allocation strategies\")\n",
    "print(\"• **Policy Enforcement**: Automatically trigger resource limits or migrations\")\n",
    "\n",
    "print(\"\\nTechnical Insights:\")\n",
    "print(\"• Isolation Forest effectively captures normal tenant behavior patterns\")\n",
    "print(\"• Network traffic features provide strong discriminative power\")\n",
    "print(\"• Unsupervised approach adapts to changing normal behavior over time\")\n",
    "print(\"• Low false positive rate makes the system practical for production use\")\n",
    "\n",
    "print(\"\\nModel Strengths:\")\n",
    "print(\"• No labeled training data required (unsupervised learning)\")\n",
    "print(\"• Scales well with number of tenants and time series length\")\n",
    "print(\"• Robust to normal variations in tenant behavior\")\n",
    "print(\"• Provides interpretable anomaly scores for ranking alerts\")\n",
    "\n",
    "print(\"\\nDeployment Considerations:\")\n",
    "print(\"• Integrate with cloud orchestration platforms (OpenStack, Kubernetes)\")\n",
    "print(\"• Set up real-time data pipelines for network metrics collection\")\n",
    "print(\"• Implement feedback loops to reduce false positives over time\")\n",
    "print(\"• Consider ensemble methods for improved robustness\")\n",
    "\n",
    "print(f\"\\nRecommendations:\")\n",
    "if recall > 0.8:\n",
    "    print(\"• Excellent recall indicates strong noisy neighbor detection capability\")\n",
    "else:\n",
    "    print(\"• Consider tuning contamination parameter or adding more features to improve recall\")\n",
    "\n",
    "if precision > 0.7:\n",
    "    print(\"• High precision indicates reliable alerts with minimal false positives\")\n",
    "else:\n",
    "    print(\"• Consider post-processing filters to reduce false positive rate\")\n",
    "\n",
    "print(\"• Deploy gradually with human oversight to validate alerts initially\")\n",
    "print(\"• Continuously monitor model performance and retrain periodically\")\n",
    "print(\"• Extend to include additional metrics like CPU, memory, and disk I/O for comprehensive detection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}