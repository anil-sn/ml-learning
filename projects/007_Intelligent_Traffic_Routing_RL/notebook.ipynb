{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 7: Intelligent Traffic Routing (Reinforcement Learning)\n",
    "\n",
    "**Objective:** To train an RL agent that can dynamically find the optimal path for network traffic from a source to a destination, minimizing total latency. The agent should also be able to adapt its path if network conditions (link latencies) change.\n",
    "\n",
    "**Environment:** We will create a simulated network environment directly in Python using the `networkx` library. This graph will represent our network, with nodes as routers/switches and edges as links with associated latency (cost).\n",
    "\n",
    "**Model:** We will implement the foundational RL algorithm, Q-Learning. The agent will learn a \"Q-table,\" which acts as a cheat sheet, telling it the expected quality (or future reward) of choosing a particular next hop from any given node.\n",
    "\n",
    "**Instructions:**\n",
    "This notebook is fully self-contained. Simply run all cells in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create the Simulated Network Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Creating Simulated Network Environment ---\")\n",
    "\n",
    "# Create a graph object\n",
    "G = nx.Graph()\n",
    "\n",
    "# Define the network topology (nodes and edges with latency as 'weight')\n",
    "edges = [\n",
    "    ('A', 'B', 7), ('A', 'C', 9), ('A', 'F', 14),\n",
    "    ('B', 'C', 10), ('B', 'D', 15),\n",
    "    ('C', 'D', 11), ('C', 'F', 2),\n",
    "    ('D', 'E', 6),\n",
    "    ('E', 'F', 9)\n",
    "]\n",
    "\n",
    "# Add edges to the graph\n",
    "for u, v, w in edges:\n",
    "    G.add_edge(u, v, weight=w)\n",
    "\n",
    "# Map node names to integers for easier array indexing\n",
    "node_map = {node: i for i, node in enumerate(G.nodes())}\n",
    "inv_node_map = {i: node for node, i in node_map.items()}\n",
    "\n",
    "print(\"Network created with the following nodes:\", list(G.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an adjacency matrix representing the latencies (costs)\n",
    "# We use a large number (np.inf) for non-existent links\n",
    "num_nodes = len(G.nodes())\n",
    "latency_matrix = np.full((num_nodes, num_nodes), np.inf)\n",
    "for u, v, data in G.edges(data=True):\n",
    "    i, j = node_map[u], node_map[v]\n",
    "    latency_matrix[i, j] = latency_matrix[j, i] = data['weight']\n",
    "\n",
    "print(f\"Created latency matrix of size {latency_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize the network\n",
    "def draw_network(graph, path=None, title=\"Network Topology\"):\n",
    "    pos = nx.spring_layout(graph, seed=42)\n",
    "    edge_labels = nx.get_edge_attributes(graph, 'weight')\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    nx.draw(graph, pos, with_labels=True, node_size=700, node_color='skyblue', font_size=10, font_weight='bold')\n",
    "    nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels)\n",
    "    if path:\n",
    "        path_edges = list(zip(path, path[1:]))\n",
    "        nx.draw_networkx_edges(graph, pos, edgelist=path_edges, edge_color='r', width=2)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "draw_network(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q-Learning Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Implementing the Q-Learning Agent ---\")\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1      # Learning rate: How much we update Q-values based on new info\n",
    "gamma = 0.9      # Discount factor: Importance of future rewards\n",
    "epsilon = 0.2    # Epsilon-greedy: Probability of exploring vs. exploiting\n",
    "num_episodes = 2000\n",
    "\n",
    "# Initialize Q-table\n",
    "# Rows are states (current node), columns are actions (next node)\n",
    "q_table = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "# The \"reward\" in our case is negative latency. The agent's goal is to\n",
    "# maximize the reward, which means minimizing the latency.\n",
    "# We set rewards for valid moves to -latency.\n",
    "rewards = -latency_matrix\n",
    "\n",
    "print(f\"Initialized Q-table with shape: {q_table.shape}\")\n",
    "print(f\"Hyperparameters: alpha={alpha}, gamma={gamma}, epsilon={epsilon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(start_node_name, end_node_name, episodes):\n",
    "    start_node = node_map[start_node_name]\n",
    "    end_node = node_map[end_node_name]\n",
    "    \n",
    "    print(f\"\\nTraining agent to find path from {start_node_name} to {end_node_name}...\")\n",
    "    for episode in range(episodes):\n",
    "        current_state = start_node\n",
    "        \n",
    "        # An episode ends when we reach the destination\n",
    "        while current_state != end_node:\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                # Explore: choose a random valid action (a connected node)\n",
    "                possible_actions = np.where(rewards[current_state] > -np.inf)[0]\n",
    "                action = random.choice(possible_actions)\n",
    "            else:\n",
    "                # Exploit: choose the best known action\n",
    "                action = np.argmax(q_table[current_state])\n",
    "\n",
    "            # Get the reward for taking that action\n",
    "            reward = rewards[current_state, action]\n",
    "\n",
    "            # Q-learning formula\n",
    "            old_value = q_table[current_state, action]\n",
    "            next_max = np.max(q_table[action]) # Best expected future reward from the next state\n",
    "            \n",
    "            # The new Q-value is a blend of the old value and the learned value\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[current_state, action] = new_value\n",
    "            \n",
    "            # Move to the next state\n",
    "            current_state = action\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "# Function to extract the optimal path from the learned Q-table\n",
    "def get_optimal_path(start_node_name, end_node_name):\n",
    "    path = [start_node_name]\n",
    "    current_node = node_map[start_node_name]\n",
    "    end_node = node_map[end_node_name]\n",
    "    \n",
    "    while current_node != end_node:\n",
    "        next_node = np.argmax(q_table[current_node])\n",
    "        path.append(inv_node_map[next_node])\n",
    "        current_node = next_node\n",
    "        if len(path) > 10: # Safety break to prevent infinite loops\n",
    "            print(\"Error: Path finding failed, stuck in a loop.\")\n",
    "            return []\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scenario 1: Find Initial Optimal Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Scenario 1: Initial Path Finding ---\")\n",
    "# Define the start and end points\n",
    "start = 'A'\n",
    "end = 'E'\n",
    "\n",
    "# Train the agent\n",
    "train_agent(start, end, num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display the optimal path\n",
    "optimal_path_1 = get_optimal_path(start, end)\n",
    "print(f\"Learned optimal path from {start} to {end}: {' -> '.join(optimal_path_1)}\")\n",
    "draw_network(G, path=optimal_path_1, title=f\"Optimal Path from {start} to {end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scenario 2: Adapt to Network Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Scenario 2: Adapting to Network Congestion ---\")\n",
    "print(\"Introducing congestion: Latency on link C -> F increases from 2 to 20.\")\n",
    "\n",
    "# Update the graph and reward matrix to reflect the change\n",
    "G['C']['F']['weight'] = 20\n",
    "rewards[node_map['C'], node_map['F']] = -20\n",
    "rewards[node_map['F'], node_map['C']] = -20\n",
    "\n",
    "# We don't need to retrain from scratch. We can continue training.\n",
    "# This allows the agent to adapt its existing knowledge.\n",
    "train_agent(start, end, num_episodes) # Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display the new optimal path\n",
    "optimal_path_2 = get_optimal_path(start, end)\n",
    "print(f\"New learned optimal path from {start} to {end}: {' -> '.join(optimal_path_2)}\")\n",
    "draw_network(G, path=optimal_path_2, title=f\"New Optimal Path after Congestion on C-F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "This notebook demonstrated the core principles of Reinforcement Learning for network routing.\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. The agent initially learned the best path by correctly identifying the low-latency C-F link.\n",
    "2. After a simulated congestion event dramatically increased latency on the C-F link, the agent adapted.\n",
    "3. By continuing its training, it discovered a new optimal route that avoids the congested link.\n",
    "\n",
    "This adaptability is the power of RL. In a real-world Software-Defined Network (SDN), an RL agent could continuously monitor link states and automatically re-route traffic to maintain optimal performance without human intervention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}