{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: DDoS Attack Detection\n",
    "\n",
    "**Objective:** To build a high-performance machine learning model that can accurately distinguish between legitimate (Benign) network traffic and malicious DDoS attack traffic based on network flow features.\n",
    "\n",
    "**Dataset Source:** CIC-DDoS2019 dataset from Kaggle - a modern and extensive dataset containing various up-to-date DDoS attack types.\n",
    "\n",
    "**Model:** RandomForestClassifier - excellent for handling large feature sets and providing interpretable results for security analysis.\n",
    "\n",
    "**Business Value:** Enables real-time threat detection, service availability protection, and automated network defense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Kaggle API and Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if kaggle.json already exists to avoid re-uploading\n",
    "if not os.path.exists('/root/.kaggle/kaggle.json'):\n",
    "    print(\"--- Setting up Kaggle API ---\")\n",
    "    !pip install -q kaggle\n",
    "    \n",
    "    # For Google Colab - prompt user to upload their kaggle.json file\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print(\"\\nPlease upload your kaggle.json file:\")\n",
    "        uploaded = files.upload()\n",
    "        if 'kaggle.json' not in uploaded:\n",
    "            print(\"\\nError: kaggle.json not uploaded.\")\n",
    "            exit()\n",
    "        print(\"\\nkaggle.json uploaded successfully.\")\n",
    "        !mkdir -p ~/.kaggle\n",
    "        !cp kaggle.json ~/.kaggle/\n",
    "        !chmod 600 ~/.kaggle/kaggle.json\n",
    "    except ImportError:\n",
    "        print(\"Not running in Google Colab. Please ensure Kaggle API is configured.\")\n",
    "        print(\"Place your kaggle.json in ~/.kaggle/ directory\")\n",
    "else:\n",
    "    print(\"Kaggle API already configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Downloading CIC-DDoS2019 Dataset from Kaggle ---\")\n",
    "print(\"‚ö†Ô∏è This is a large dataset. The download may take several minutes.\")\n",
    "\n",
    "# Download the CIC-DDoS2019 dataset\n",
    "!kaggle datasets download -d frazane/cicddos2019\n",
    "\n",
    "print(\"\\n--- Unzipping the dataset ---\")\n",
    "# The dataset is composed of multiple large files\n",
    "!unzip -q cicddos2019.zip -d cicddos2019\n",
    "print(\"Dataset setup complete.\")\n",
    "\n",
    "# List the contents to understand the structure\n",
    "print(\"\\nDataset structure:\")\n",
    "!ls -la cicddos2019/\n",
    "!ls -la cicddos2019/CSVs/ | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, \n",
    "    precision_recall_curve, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Loading and Preprocessing Data ---\")\n",
    "\n",
    "# The dataset is split into multiple CSVs. We'll load a representative sample\n",
    "# to keep memory usage manageable while demonstrating the full pipeline\n",
    "path = 'cicddos2019/CSVs'\n",
    "\n",
    "# Load specific files that contain diverse attack types and benign traffic\n",
    "filenames = [\n",
    "    os.path.join(path, 'DrDoS_NTP.csv'),      # NTP reflection attack\n",
    "    os.path.join(path, 'syn_and_benign.csv')  # SYN flood + benign traffic\n",
    "]\n",
    "\n",
    "# Check if files exist and load them\n",
    "df_list = []\n",
    "for filename in filenames:\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Loading {filename}...\")\n",
    "        df_temp = pd.read_csv(filename)\n",
    "        print(f\"  - Shape: {df_temp.shape}\")\n",
    "        print(f\"  - Labels: {df_temp['Label'].value_counts().to_dict()}\")\n",
    "        df_list.append(df_temp)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è File not found: {filename}\")\n",
    "\n",
    "if len(df_list) == 0:\n",
    "    print(\"‚ùå No data files found. Please check the dataset extraction.\")\n",
    "    # List available files for debugging\n",
    "    print(\"Available files:\")\n",
    "    !ls cicddos2019/CSVs/\n",
    "    exit()\n",
    "\n",
    "# Combine all loaded dataframes\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "print(f\"\\n‚úÖ Successfully loaded {len(df_list)} files.\")\n",
    "print(f\"Combined dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data exploration\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\n=== COLUMN INFO ===\")\n",
    "print(df.info())\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\n=== SAMPLE DATA ===\")\n",
    "print(df.head())\n",
    "\n",
    "# Examine target variable\n",
    "print(\"\\n=== LABEL DISTRIBUTION ===\")\n",
    "label_counts = df['Label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "# Calculate class balance\n",
    "total_samples = len(df)\n",
    "print(f\"\\nClass Balance:\")\n",
    "for label, count in label_counts.items():\n",
    "    percentage = (count / total_samples) * 100\n",
    "    print(f\"  {label}: {count:,} samples ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar chart of label counts\n",
    "label_counts.plot(kind='bar', ax=axes[0], alpha=0.8)\n",
    "axes[0].set_title('Distribution of Traffic Types', fontweight='bold', fontsize=14)\n",
    "axes[0].set_xlabel('Traffic Type')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pie chart for proportions\n",
    "axes[1].pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Proportion of Traffic Types', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df.head(10))\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA CLEANING PIPELINE ===\")\n",
    "\n",
    "# Create a copy for preprocessing\n",
    "df_clean = df.copy()\n",
    "print(f\"Starting shape: {df_clean.shape}\")\n",
    "\n",
    "# Step 1: Clean column names (remove leading/trailing spaces)\n",
    "df_clean.columns = df_clean.columns.str.strip()\n",
    "print(\"‚úÖ Cleaned column names\")\n",
    "\n",
    "# Step 2: Remove non-predictive columns\n",
    "# These columns are either identifiers or have issues in some datasets\n",
    "columns_to_drop = ['Unnamed: 0', 'Flow ID', 'Source IP', 'Destination IP', 'Timestamp']\n",
    "\n",
    "# Also check for duplicate header column (common in this dataset)\n",
    "if 'Fwd Header Length.1' in df_clean.columns:\n",
    "    columns_to_drop.append('Fwd Header Length.1')\n",
    "\n",
    "# Drop columns that exist\n",
    "existing_drops = [col for col in columns_to_drop if col in df_clean.columns]\n",
    "df_clean = df_clean.drop(columns=existing_drops)\n",
    "print(f\"‚úÖ Dropped {len(existing_drops)} identifier columns: {existing_drops}\")\n",
    "\n",
    "# Step 3: Handle infinite values and NaNs\n",
    "# These can occur from division-by-zero in feature calculations\n",
    "print(\"\\nHandling infinite and NaN values...\")\n",
    "print(f\"Infinite values found: {np.isinf(df_clean.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Replace infinite values with NaN, then drop\n",
    "df_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "rows_before = len(df_clean)\n",
    "df_clean.dropna(inplace=True)\n",
    "rows_after = len(df_clean)\n",
    "\n",
    "print(f\"‚úÖ Removed {rows_before - rows_after} rows with NaN/infinite values\")\n",
    "print(f\"Final shape after cleaning: {df_clean.shape}\")\n",
    "\n",
    "# Step 4: Encode labels\n",
    "print(\"\\nEncoding labels...\")\n",
    "print(f\"Original labels: {df_clean['Label'].unique()}\")\n",
    "\n",
    "# Convert to binary classification: Benign = 0, Any attack = 1\n",
    "df_clean['Label'] = df_clean['Label'].apply(lambda x: 0 if x == 'Benign' else 1)\n",
    "print(f\"Encoded labels: {df_clean['Label'].unique()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Data preprocessing complete!\")\n",
    "print(f\"Final dataset shape: {df_clean.shape}\")\n",
    "print(\"\\nFinal label distribution:\")\n",
    "print(df_clean['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Analysis and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key features that distinguish between benign and malicious traffic\n",
    "print(\"=== FEATURE ANALYSIS ===\")\n",
    "\n",
    "# Separate features and target\n",
    "feature_columns = df_clean.columns.drop('Label')\n",
    "print(f\"Total features available: {len(feature_columns)}\")\n",
    "\n",
    "# Sample some key features for analysis\n",
    "key_features = [\n",
    "    'Total Length of Fwd Packets', 'Total Length of Bwd Packets',\n",
    "    'Bwd Packet Length Mean', 'Flow Duration', 'Avg Fwd Segment Size',\n",
    "    'Fwd Packets/s', 'Bwd Packets/s', 'Idle Mean'\n",
    "]\n",
    "\n",
    "# Select features that exist in our dataset\n",
    "available_key_features = [f for f in key_features if f in feature_columns]\n",
    "print(f\"Key features for analysis: {len(available_key_features)}\")\n",
    "\n",
    "if len(available_key_features) >= 4:\n",
    "    # Create comparison plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, feature in enumerate(available_key_features[:4]):\n",
    "        # Box plot comparing benign vs malicious traffic\n",
    "        df_sample = df_clean.sample(min(10000, len(df_clean)))  # Sample for visualization\n",
    "        sns.boxplot(data=df_sample, x='Label', y=feature, ax=axes[i])\n",
    "        axes[i].set_title(f'Distribution of {feature}', fontweight='bold')\n",
    "        axes[i].set_xlabel('Traffic Type (0=Benign, 1=Attack)')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical comparison\n",
    "    print(\"\\nStatistical Comparison (Benign vs Attack):\")\n",
    "    for feature in available_key_features[:5]:\n",
    "        benign_mean = df_clean[df_clean['Label'] == 0][feature].mean()\n",
    "        attack_mean = df_clean[df_clean['Label'] == 1][feature].mean()\n",
    "        print(f\"{feature}:\")\n",
    "        print(f\"  Benign avg: {benign_mean:.4f}\")\n",
    "        print(f\"  Attack avg: {attack_mean:.4f}\")\n",
    "        print(f\"  Ratio: {attack_mean/benign_mean if benign_mean != 0 else 'inf':.2f}\")\n",
    "        print()\n",
    "\n",
    "# Check for any remaining data quality issues\n",
    "print(\"\\n=== FINAL DATA QUALITY CHECK ===\")\n",
    "numeric_features = df_clean.select_dtypes(include=[np.number]).columns.drop('Label')\n",
    "print(f\"Numeric features: {len(numeric_features)}\")\n",
    "print(f\"Any infinite values: {np.isinf(df_clean[numeric_features]).sum().sum()}\")\n",
    "print(f\"Any NaN values: {df_clean[numeric_features].isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train-Test Split and Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PREPARING DATA FOR TRAINING ===\")\n",
    "\n",
    "# Separate features (X) from the target label (y)\n",
    "X = df_clean.drop(columns=['Label'])\n",
    "y = df_clean['Label']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Features: {X.columns.tolist()[:10]}...\")  # Show first 10 features\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# stratify=y ensures balanced representation in both train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n=== TRAIN-TEST SPLIT RESULTS ===\")\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Verify class balance is maintained\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "train_dist = y_train.value_counts()\n",
    "for label, count in train_dist.items():\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    label_name = 'Benign' if label == 0 else 'Attack'\n",
    "    print(f\"  {label_name} ({label}): {count:,} samples ({percentage:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "test_dist = y_test.value_counts()\n",
    "for label, count in test_dist.items():\n",
    "    percentage = (count / len(y_test)) * 100\n",
    "    label_name = 'Benign' if label == 0 else 'Attack'\n",
    "    print(f\"  {label_name} ({label}): {count:,} samples ({percentage:.2f}%)\")\n",
    "\n",
    "# Memory usage check\n",
    "print(f\"\\nMemory usage:\")\n",
    "print(f\"  Training features: {X_train.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  Test features: {X_test.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== RANDOM FOREST TRAINING ===\")\n",
    "\n",
    "# Initialize the RandomForestClassifier with optimized parameters\n",
    "# n_estimators=100: Good balance between performance and training time\n",
    "# max_depth=None: Allow trees to grow deep for complex patterns\n",
    "# min_samples_split=5: Prevent overfitting\n",
    "# min_samples_leaf=2: Ensure sufficient samples in leaf nodes\n",
    "# n_jobs=-1: Use all available CPU cores\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1  # Show progress\n",
    ")\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Algorithm: Random Forest\")\n",
    "print(f\"  Number of trees: {model.n_estimators}\")\n",
    "print(f\"  Max depth: {model.max_depth}\")\n",
    "print(f\"  Min samples split: {model.min_samples_split}\")\n",
    "print(f\"  Min samples leaf: {model.min_samples_leaf}\")\n",
    "\n",
    "print(f\"\\nTraining on {X_train.shape[0]:,} samples with {X_train.shape[1]} features...\")\n",
    "print(\"This may take a few minutes depending on your hardware...\")\n",
    "\n",
    "# Train the model\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "\n",
    "# Display model information\n",
    "print(f\"\\n=== TRAINED MODEL INFO ===\")\n",
    "print(f\"Number of features used: {model.n_features_in_}\")\n",
    "print(f\"Number of classes: {len(model.classes_)}\")\n",
    "print(f\"Classes: {model.classes_} (0=Benign, 1=Attack)\")\n",
    "print(f\"Number of trees: {len(model.estimators_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL EVALUATION ===\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "print(\"Generating predictions...\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of attack class\n",
    "\n",
    "# Calculate basic accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nüìä Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\n=== DETAILED CLASSIFICATION REPORT ===\")\n",
    "class_names = ['Benign (0)', 'Attack (1)']\n",
    "report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "# Extract key metrics for security analysis\n",
    "benign_precision = report['Benign (0)']['precision']\n",
    "benign_recall = report['Benign (0)']['recall']\n",
    "attack_precision = report['Attack (1)']['precision']\n",
    "attack_recall = report['Attack (1)']['recall']\n",
    "\n",
    "print(f\"\\n=== SECURITY METRICS INTERPRETATION ===\")\n",
    "print(f\"Attack Detection Precision: {attack_precision:.4f}\")\n",
    "print(f\"  ‚Üí When model predicts attack, it's correct {attack_precision*100:.1f}% of the time\")\n",
    "print(f\"Attack Detection Recall: {attack_recall:.4f}\")\n",
    "print(f\"  ‚Üí Model catches {attack_recall*100:.1f}% of actual attacks\")\n",
    "print(f\"False Positive Rate: {1-benign_recall:.4f}\")\n",
    "print(f\"  ‚Üí {(1-benign_recall)*100:.1f}% of benign traffic flagged as attacks\")\n",
    "print(f\"False Negative Rate: {1-attack_recall:.4f}\")\n",
    "print(f\"  ‚Üí {(1-attack_recall)*100:.1f}% of attacks go undetected\")\n",
    "\n",
    "# Calculate AUC-ROC for overall model performance\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"\\nAUC-ROC Score: {auc_score:.4f}\")\n",
    "if auc_score > 0.95:\n",
    "    performance_level = \"Excellent\"\n",
    "elif auc_score > 0.90:\n",
    "    performance_level = \"Very Good\"\n",
    "elif auc_score > 0.85:\n",
    "    performance_level = \"Good\"\n",
    "else:\n",
    "    performance_level = \"Needs Improvement\"\n",
    "print(f\"Performance Level: {performance_level}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Analysis\n",
    "print(\"\\n=== CONFUSION MATRIX ANALYSIS ===\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate detailed confusion matrix metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives (TN): {tn:,} - Correctly identified benign traffic\")\n",
    "print(f\"  False Positives (FP): {fp:,} - Benign traffic flagged as attacks\")\n",
    "print(f\"  False Negatives (FN): {fn:,} - Missed attacks\")\n",
    "print(f\"  True Positives (TP): {tp:,} - Correctly detected attacks\")\n",
    "\n",
    "# Create enhanced confusion matrix visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create annotated heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted Benign', 'Predicted Attack'], \n",
    "            yticklabels=['Actual Benign', 'Actual Attack'],\n",
    "            cbar_kws={'label': 'Number of Samples'})\n",
    "\n",
    "plt.title('DDoS Attack Detection - Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('Actual Class', fontsize=12)\n",
    "plt.xlabel('Predicted Class', fontsize=12)\n",
    "\n",
    "# Add percentage annotations\n",
    "total = np.sum(cm)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        percentage = cm[i, j] / total * 100\n",
    "        plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', \n",
    "                horizontalalignment='center', fontsize=10, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Business impact analysis\n",
    "print(f\"\\n=== BUSINESS IMPACT ANALYSIS ===\")\n",
    "total_traffic = len(y_test)\n",
    "print(f\"Total network traffic analyzed: {total_traffic:,} flows\")\n",
    "print(f\"\")\n",
    "print(f\"Security Effectiveness:\")\n",
    "print(f\"  ‚úÖ Successfully blocked: {tp:,} attack flows ({tp/total_traffic*100:.2f}% of total)\")\n",
    "print(f\"  ‚ùå Missed attacks: {fn:,} flows ({fn/total_traffic*100:.3f}% of total)\")\n",
    "print(f\"  ‚ö†Ô∏è  False alarms: {fp:,} flows ({fp/total_traffic*100:.3f}% of total)\")\n",
    "print(f\"  ‚úÖ Correctly allowed: {tn:,} benign flows ({tn/total_traffic*100:.2f}% of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ROC Curve and Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve and Precision-Recall Curve Analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "axes[0].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "axes[0].set_xlim([0.0, 1.0])\n",
    "axes[0].set_ylim([0.0, 1.05])\n",
    "axes[0].set_xlabel('False Positive Rate (1 - Specificity)')\n",
    "axes[0].set_ylabel('True Positive Rate (Sensitivity)')\n",
    "axes[0].set_title('ROC Curve - DDoS Attack Detection', fontweight='bold')\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "avg_precision = np.mean(precision)\n",
    "axes[1].plot(recall, precision, linewidth=2, label=f'PR Curve (AP = {avg_precision:.3f})')\n",
    "axes[1].axhline(y=np.mean(y_test), color='k', linestyle='--', linewidth=1, \n",
    "               label=f'Random Classifier (AP = {np.mean(y_test):.3f})')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('Recall (True Positive Rate)')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve - DDoS Attack Detection', fontweight='bold')\n",
    "axes[1].legend(loc=\"lower left\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== CURVE ANALYSIS ===\")\n",
    "print(f\"ROC-AUC: {auc_score:.4f}\")\n",
    "print(f\"  ‚Üí Measures overall discriminative ability\")\n",
    "print(f\"  ‚Üí Values closer to 1.0 indicate better performance\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"  ‚Üí Particularly important for imbalanced datasets\")\n",
    "print(f\"  ‚Üí Focuses on precision at different recall levels\")\n",
    "\n",
    "# Threshold analysis for operational deployment\n",
    "print(f\"\\n=== THRESHOLD ANALYSIS FOR DEPLOYMENT ===\")\n",
    "thresholds = [0.3, 0.5, 0.7, 0.9]\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
    "    cm_thresh = confusion_matrix(y_test, y_pred_thresh)\n",
    "    tn_t, fp_t, fn_t, tp_t = cm_thresh.ravel()\n",
    "    \n",
    "    precision_t = tp_t / (tp_t + fp_t) if (tp_t + fp_t) > 0 else 0\n",
    "    recall_t = tp_t / (tp_t + fn_t) if (tp_t + fn_t) > 0 else 0\n",
    "    fpr_t = fp_t / (fp_t + tn_t) if (fp_t + tn_t) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nThreshold {threshold}:\")\n",
    "    print(f\"  Attack Detection Rate: {recall_t:.3f} ({recall_t*100:.1f}%)\")\n",
    "    print(f\"  False Positive Rate: {fpr_t:.4f} ({fpr_t*100:.2f}%)\")\n",
    "    print(f\"  Precision: {precision_t:.3f} ({precision_t*100:.1f}%)\")\n",
    "    \n",
    "    if threshold == 0.5:\n",
    "        print(f\"  ‚Üê Default threshold used in evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "\n",
    "# Extract feature importances from the trained model\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 15 Most Important Features:\")\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "# Visualize top 15 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "\n",
    "# Create horizontal bar plot\n",
    "bars = plt.barh(range(len(top_features)), top_features['importance'], alpha=0.8)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance Score', fontsize=12)\n",
    "plt.title('Top 15 Features for DDoS Attack Detection', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()  # Highest importance at top\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.3f}', ha='left', va='center', fontsize=10)\n",
    "\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Categorize features for network engineering insights\n",
    "print(f\"\\n=== NETWORK ENGINEERING INSIGHTS ===\")\n",
    "\n",
    "# Group features by category\n",
    "timing_features = [f for f in top_features['feature'] if any(keyword in f.lower() \n",
    "                   for keyword in ['time', 'duration', 'iat', 'idle'])]\n",
    "packet_features = [f for f in top_features['feature'] if any(keyword in f.lower() \n",
    "                   for keyword in ['packet', 'length', 'size'])]\n",
    "flow_features = [f for f in top_features['feature'] if any(keyword in f.lower() \n",
    "                 for keyword in ['flow', 'rate', '/s'])]\n",
    "flag_features = [f for f in top_features['feature'] if any(keyword in f.lower() \n",
    "                 for keyword in ['flag', 'tcp'])]\n",
    "\n",
    "print(f\"Key Timing-based Features ({len(timing_features)}):\")\n",
    "for feature in timing_features[:5]:\n",
    "    importance = feature_importance[feature_importance['feature'] == feature]['importance'].iloc[0]\n",
    "    print(f\"  ‚Ä¢ {feature}: {importance:.4f}\")\n",
    "\n",
    "print(f\"\\nKey Packet-based Features ({len(packet_features)}):\")\n",
    "for feature in packet_features[:5]:\n",
    "    importance = feature_importance[feature_importance['feature'] == feature]['importance'].iloc[0]\n",
    "    print(f\"  ‚Ä¢ {feature}: {importance:.4f}\")\n",
    "\n",
    "print(f\"\\nKey Flow-based Features ({len(flow_features)}):\")\n",
    "for feature in flow_features[:5]:\n",
    "    importance = feature_importance[feature_importance['feature'] == feature]['importance'].iloc[0]\n",
    "    print(f\"  ‚Ä¢ {feature}: {importance:.4f}\")\n",
    "\n",
    "# Summary insights for network operations\n",
    "print(f\"\\n=== OPERATIONAL INSIGHTS ===\")\n",
    "print(f\"1. TIMING PATTERNS: DDoS attacks show distinct timing signatures\")\n",
    "print(f\"   - Monitor inter-arrival times and flow durations\")\n",
    "print(f\"   - Abnormal timing patterns indicate potential attacks\")\n",
    "\n",
    "print(f\"\\n2. PACKET CHARACTERISTICS: Attack packets have different size profiles\")\n",
    "print(f\"   - Mean packet lengths vary significantly between benign/malicious\")\n",
    "print(f\"   - Packet size distributions are key indicators\")\n",
    "\n",
    "print(f\"\\n3. VOLUME METRICS: Rate-based features are crucial\")\n",
    "print(f\"   - Packets per second and bytes per second patterns\")\n",
    "print(f\"   - Sudden volume spikes indicate potential DDoS\")\n",
    "\n",
    "# Export feature importance for operational use\n",
    "feature_importance.to_csv('ddos_feature_importance.csv', index=False)\n",
    "print(f\"\\n‚úÖ Feature importance exported to 'ddos_feature_importance.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Validation and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL VALIDATION ===\")\n",
    "\n",
    "# Perform cross-validation to ensure model robustness\n",
    "print(\"Performing 5-fold cross-validation...\")\n",
    "print(\"This provides a more robust estimate of model performance.\")\n",
    "\n",
    "# Use a smaller subset for cross-validation to manage computational time\n",
    "# In production, you would use the full dataset\n",
    "cv_sample_size = min(20000, len(X))\n",
    "X_cv = X.sample(cv_sample_size, random_state=42)\n",
    "y_cv = y.loc[X_cv.index]\n",
    "\n",
    "print(f\"Cross-validation on {cv_sample_size:,} samples...\")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X_cv, y_cv, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "cv_precision = cross_val_score(model, X_cv, y_cv, cv=5, scoring='precision', n_jobs=-1)\n",
    "cv_recall = cross_val_score(model, X_cv, y_cv, cv=5, scoring='recall', n_jobs=-1)\n",
    "cv_f1 = cross_val_score(model, X_cv, y_cv, cv=5, scoring='f1', n_jobs=-1)\n",
    "\n",
    "print(f\"\\n=== CROSS-VALIDATION RESULTS ===\")\n",
    "print(f\"Accuracy: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "print(f\"  Individual folds: {cv_scores}\")\n",
    "print(f\"Precision: {cv_precision.mean():.4f} ¬± {cv_precision.std():.4f}\")\n",
    "print(f\"Recall: {cv_recall.mean():.4f} ¬± {cv_recall.std():.4f}\")\n",
    "print(f\"F1-Score: {cv_f1.mean():.4f} ¬± {cv_f1.std():.4f}\")\n",
    "\n",
    "# Stability analysis\n",
    "if cv_scores.std() < 0.01:\n",
    "    stability = \"Very Stable\"\n",
    "elif cv_scores.std() < 0.02:\n",
    "    stability = \"Stable\"\n",
    "else:\n",
    "    stability = \"Moderately Stable\"\n",
    "\n",
    "print(f\"\\nModel Stability: {stability}\")\n",
    "print(f\"(Standard deviation of CV scores: {cv_scores.std():.4f})\")\n",
    "\n",
    "# Performance consistency check\n",
    "print(f\"\\n=== PERFORMANCE CONSISTENCY ===\")\n",
    "print(f\"Minimum accuracy across folds: {cv_scores.min():.4f}\")\n",
    "print(f\"Maximum accuracy across folds: {cv_scores.max():.4f}\")\n",
    "print(f\"Range: {cv_scores.max() - cv_scores.min():.4f}\")\n",
    "\n",
    "if (cv_scores.max() - cv_scores.min()) < 0.02:\n",
    "    consistency = \"Highly Consistent\"\n",
    "elif (cv_scores.max() - cv_scores.min()) < 0.05:\n",
    "    consistency = \"Consistent\"\n",
    "else:\n",
    "    consistency = \"Moderately Consistent\"\n",
    "\n",
    "print(f\"Performance Consistency: {consistency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Deployment Readiness Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DEPLOYMENT READINESS ASSESSMENT ===\")\n",
    "\n",
    "# Assess model prediction speed for real-time deployment\n",
    "import time\n",
    "\n",
    "# Test prediction speed on various batch sizes\n",
    "batch_sizes = [1, 10, 100, 1000]\n",
    "prediction_times = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    if batch_size <= len(X_test):\n",
    "        sample_data = X_test.iloc[:batch_size]\n",
    "        \n",
    "        # Time the prediction\n",
    "        start_time = time.time()\n",
    "        predictions = model.predict(sample_data)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        prediction_time = (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "        per_sample_time = prediction_time / batch_size\n",
    "        \n",
    "        prediction_times[batch_size] = {\n",
    "            'total_ms': prediction_time,\n",
    "            'per_sample_ms': per_sample_time,\n",
    "            'samples_per_second': 1000 / per_sample_time\n",
    "        }\n",
    "\n",
    "print(f\"\\n=== PREDICTION SPEED ANALYSIS ===\")\n",
    "for batch_size, times in prediction_times.items():\n",
    "    print(f\"Batch size {batch_size}:\")\n",
    "    print(f\"  Total time: {times['total_ms']:.2f} ms\")\n",
    "    print(f\"  Per sample: {times['per_sample_ms']:.3f} ms\")\n",
    "    print(f\"  Throughput: {times['samples_per_second']:.0f} samples/second\")\n",
    "    print()\n",
    "\n",
    "# Real-time deployment assessment\n",
    "single_prediction_time = prediction_times[1]['per_sample_ms']\n",
    "if single_prediction_time < 1:\n",
    "    deployment_readiness = \"Excellent for real-time deployment\"\n",
    "elif single_prediction_time < 10:\n",
    "    deployment_readiness = \"Good for near real-time deployment\"\n",
    "elif single_prediction_time < 100:\n",
    "    deployment_readiness = \"Suitable for batch processing\"\n",
    "else:\n",
    "    deployment_readiness = \"May need optimization for production\"\n",
    "\n",
    "print(f\"\\n=== DEPLOYMENT RECOMMENDATION ===\")\n",
    "print(f\"Single prediction time: {single_prediction_time:.3f} ms\")\n",
    "print(f\"Assessment: {deployment_readiness}\")\n",
    "\n",
    "# Memory requirements\n",
    "import sys\n",
    "model_size = sys.getsizeof(model) / 1024**2  # Convert to MB\n",
    "print(f\"\\nModel memory footprint: {model_size:.2f} MB\")\n",
    "\n",
    "# Feature requirements for deployment\n",
    "print(f\"\\n=== DEPLOYMENT REQUIREMENTS ===\")\n",
    "print(f\"‚úÖ Required features: {len(X.columns)}\")\n",
    "print(f\"‚úÖ Preprocessing steps: Handle inf/NaN, drop identifier columns\")\n",
    "print(f\"‚úÖ Input format: Pandas DataFrame or NumPy array\")\n",
    "print(f\"‚úÖ Output: Binary classification (0=Benign, 1=Attack)\")\n",
    "print(f\"‚úÖ Probability scores: Available via predict_proba()\")\n",
    "\n",
    "# Save model for deployment\n",
    "import joblib\n",
    "joblib.dump(model, 'ddos_detection_model.pkl')\n",
    "print(f\"\\n‚úÖ Model saved as 'ddos_detection_model.pkl' for deployment\")\n",
    "\n",
    "# Create deployment metadata\n",
    "deployment_metadata = {\n",
    "    'model_type': 'RandomForestClassifier',\n",
    "    'model_version': '1.0',\n",
    "    'training_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_accuracy': float(accuracy),\n",
    "    'test_precision': float(attack_precision),\n",
    "    'test_recall': float(attack_recall),\n",
    "    'auc_score': float(auc_score),\n",
    "    'feature_count': len(X.columns),\n",
    "    'prediction_time_ms': float(single_prediction_time),\n",
    "    'required_features': X.columns.tolist()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('model_metadata.json', 'w') as f:\n",
    "    json.dump(deployment_metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Deployment metadata saved as 'model_metadata.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Security and Business Impact Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"        DDOS ATTACK DETECTION - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüõ°Ô∏è SECURITY PERFORMANCE:\")\n",
    "print(f\"   ‚Ä¢ Overall Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Attack Detection Rate: {attack_recall*100:.2f}% (Recall)\")\n",
    "print(f\"   ‚Ä¢ Attack Precision: {attack_precision*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ False Positive Rate: {(1-benign_recall)*100:.3f}%\")\n",
    "print(f\"   ‚Ä¢ AUC-ROC Score: {auc_score:.4f} - {performance_level}\")\n",
    "\n",
    "print(\"\\nüìä OPERATIONAL IMPACT:\")\n",
    "attacks_detected = tp\n",
    "attacks_missed = fn\n",
    "false_alarms = fp\n",
    "total_attacks = tp + fn\n",
    "total_benign = tn + fp\n",
    "\n",
    "print(f\"   ‚Ä¢ Attacks Successfully Blocked: {attacks_detected:,} / {total_attacks:,}\")\n",
    "print(f\"   ‚Ä¢ Critical Attacks Missed: {attacks_missed:,}\")\n",
    "print(f\"   ‚Ä¢ False Alarms Generated: {false_alarms:,} / {total_benign:,}\")\n",
    "print(f\"   ‚Ä¢ Prediction Speed: {single_prediction_time:.3f} ms per flow\")\n",
    "print(f\"   ‚Ä¢ Throughput: {prediction_times[1]['samples_per_second']:.0f} flows/second\")\n",
    "\n",
    "print(\"\\nüéØ KEY ATTACK INDICATORS:\")\n",
    "top_3_features = feature_importance.head(3)\n",
    "for idx, row in top_3_features.iterrows():\n",
    "    print(f\"   ‚Ä¢ {row['feature']}: {row['importance']:.4f} importance\")\n",
    "\n",
    "print(\"\\nüíº BUSINESS VALUE:\")\n",
    "if attack_recall > 0.95 and (1-benign_recall) < 0.05:\n",
    "    business_value = \"HIGH - Excellent protection with minimal disruption\"\n",
    "elif attack_recall > 0.90 and (1-benign_recall) < 0.10:\n",
    "    business_value = \"GOOD - Strong protection with acceptable false positives\"\n",
    "else:\n",
    "    business_value = \"MODERATE - May need threshold tuning for optimal balance\"\n",
    "\n",
    "print(f\"   ‚Ä¢ Protection Level: {business_value}\")\n",
    "print(f\"   ‚Ä¢ Automated Defense: ‚úÖ Ready for real-time deployment\")\n",
    "print(f\"   ‚Ä¢ Cost Reduction: ‚úÖ Prevents service outages and emergency response\")\n",
    "print(f\"   ‚Ä¢ Compliance: ‚úÖ Meets security monitoring requirements\")\n",
    "\n",
    "print(\"\\nüîß DEPLOYMENT RECOMMENDATIONS:\")\n",
    "print(f\"   ‚úì Deploy with default threshold (0.5) for balanced performance\")\n",
    "if (1-benign_recall) > 0.02:\n",
    "    print(f\"   ‚ö†Ô∏è Consider higher threshold (0.7) to reduce false positives\")\n",
    "if attack_recall < 0.95:\n",
    "    print(f\"   ‚ö†Ô∏è Consider lower threshold (0.3) for maximum attack detection\")\n",
    "print(f\"   ‚úì Implement automated blocking for high-confidence predictions (>0.8)\")\n",
    "print(f\"   ‚úì Set up alerts for predictions between 0.5-0.8 for human review\")\n",
    "print(f\"   ‚úì Regular model retraining recommended (monthly with new attack data)\")\n",
    "\n",
    "print(\"\\nüìÅ GENERATED ARTIFACTS:\")\n",
    "print(f\"   ‚Ä¢ ddos_detection_model.pkl - Trained model for deployment\")\n",
    "print(f\"   ‚Ä¢ model_metadata.json - Deployment configuration\")\n",
    "print(f\"   ‚Ä¢ ddos_feature_importance.csv - Feature analysis for SOC teams\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"    MODEL READY FOR PRODUCTION DEPLOYMENT\")\n",
    "print(f\"    Trained on {len(X_train):,} flows | Validated on {len(X_test):,} flows\")\n",
    "print(f\"    Attack Detection: {attack_recall*100:.1f}% | False Alarms: {(1-benign_recall)*100:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"   1. Integrate model into network security infrastructure\")\n",
    "print(\"   2. Set up monitoring dashboard for prediction statistics\")\n",
    "print(\"   3. Implement automated response for high-confidence attacks\")\n",
    "print(\"   4. Plan regular model updates with new threat intelligence\")\n",
    "print(\"   5. Conduct A/B testing in production environment\")\n",
    "\n",
    "print(\"\\n‚úÖ DDoS Attack Detection Model Training Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}