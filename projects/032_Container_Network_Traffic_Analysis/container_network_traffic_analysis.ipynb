{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "project-header",
   "metadata": {},
   "source": [
    "# Project 32: Container Network Traffic Pattern Analysis\n",
    "\n",
    "**Objective:** Build a machine learning model that can classify the type of application running inside a container (e.g., 'WebApp', 'Database', 'Cache') by analyzing the statistical features of its network traffic.\n",
    "\n",
    "**Dataset Source:** Synthetically Generated (simulated network flow data from different containerized applications)\n",
    "\n",
    "**Model:** RandomForestClassifier for learning complex patterns that differentiate network behavior of various applications\n",
    "\n",
    "**Instructions:**\n",
    "This notebook is fully self-contained and does not require external files. Simply run all cells in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Project 32: Container Network Traffic Analysis - Setup and Imports\n",
    "# ==================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Synthetic Container Traffic Data Generation\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Generating Synthetic Container Network Traffic Dataset ---\")\n",
    "\n",
    "num_samples = 5000\n",
    "data = []\n",
    "app_types = ['WebApp', 'Database', 'Cache', 'MessageQueue', 'APIGateway']\n",
    "\n",
    "# Define the \"network personality\" of each application\n",
    "app_profiles = {\n",
    "    'WebApp':       {'avg_pkt_size': 500,  'server_port': 443,  'flow_duration_ms': 500, 'client_server_ratio': 0.8},\n",
    "    'Database':     {'avg_pkt_size': 1000, 'server_port': 5432, 'flow_duration_ms': 100, 'client_server_ratio': 0.5},\n",
    "    'Cache':        {'avg_pkt_size': 150,  'server_port': 6379, 'flow_duration_ms': 20,  'client_server_ratio': 0.5},\n",
    "    'MessageQueue': {'avg_pkt_size': 300,  'server_port': 5672, 'flow_duration_ms': 10000,'client_server_ratio': 0.5},\n",
    "    'APIGateway':   {'avg_pkt_size': 800,  'server_port': 8080, 'flow_duration_ms': 200, 'client_server_ratio': 0.7}\n",
    "}\n",
    "\n",
    "print(\"\\nApplication Network Profiles:\")\n",
    "for app, profile in app_profiles.items():\n",
    "    print(f\"{app:12}: avg_pkt={profile['avg_pkt_size']:4}B, port={profile['server_port']:4}, duration={profile['flow_duration_ms']:5}ms, ratio={profile['client_server_ratio']:.1f}\")\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    app_type = random.choice(app_types)\n",
    "    profile = app_profiles[app_type]\n",
    "    \n",
    "    # Generate features based on the profile with some randomness\n",
    "    avg_pkt_size = max(50, np.random.normal(profile['avg_pkt_size'], 50))\n",
    "    server_port = profile['server_port']\n",
    "    flow_duration_ms = max(1, np.random.normal(profile['flow_duration_ms'], 100))\n",
    "    # Ratio of packets sent by client vs. server\n",
    "    client_server_ratio = np.clip(np.random.normal(profile['client_server_ratio'], 0.1), 0.1, 0.9)\n",
    "    \n",
    "    # Number of packets in the flow\n",
    "    total_packets = np.random.randint(5, 100)\n",
    "    client_packets = int(total_packets * client_server_ratio)\n",
    "    server_packets = total_packets - client_packets\n",
    "    \n",
    "    # Additional derived features\n",
    "    total_bytes = avg_pkt_size * total_packets\n",
    "    throughput_kbps = (total_bytes * 8) / (flow_duration_ms / 1000) / 1000  # Kbps\n",
    "    \n",
    "    data.append([avg_pkt_size, server_port, flow_duration_ms, client_packets, \n",
    "                server_packets, total_bytes, throughput_kbps, app_type])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['avg_pkt_size', 'server_port', 'flow_duration_ms', \n",
    "                                'client_packets', 'server_packets', 'total_bytes', \n",
    "                                'throughput_kbps', 'app_type'])\n",
    "\n",
    "# Remove any anomalous data\n",
    "df = df[(df['flow_duration_ms'] > 0) & (df['throughput_kbps'] > 0)]\n",
    "\n",
    "print(f\"\\nDataset generation complete. Created {len(df)} flow samples.\")\n",
    "print(f\"Features: {list(df.columns[:-1])}\")\n",
    "print(f\"Target: {df.columns[-1]}\")\n",
    "\n",
    "print(\"\\nDataset Sample:\")\n",
    "print(df.sample(10).round(2))\n",
    "\n",
    "print(\"\\nApplication Type Distribution:\")\n",
    "print(df['app_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Data Exploration and Visualization\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Data Exploration and Pattern Analysis ---\")\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(df.describe().round(2))\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Container Application Network Traffic Patterns', fontsize=16)\n",
    "\n",
    "# 1. Average packet size by application type\n",
    "sns.boxplot(data=df, x='app_type', y='avg_pkt_size', ax=axes[0,0])\n",
    "axes[0,0].set_title('Average Packet Size by Application')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Flow duration by application type\n",
    "sns.boxplot(data=df, x='app_type', y='flow_duration_ms', ax=axes[0,1])\n",
    "axes[0,1].set_title('Flow Duration by Application')\n",
    "axes[0,1].set_yscale('log')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Throughput by application type\n",
    "sns.boxplot(data=df, x='app_type', y='throughput_kbps', ax=axes[0,2])\n",
    "axes[0,2].set_title('Throughput by Application')\n",
    "axes[0,2].set_yscale('log')\n",
    "axes[0,2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Client vs Server packets scatter\n",
    "for app in app_types:\n",
    "    app_data = df[df['app_type'] == app]\n",
    "    axes[1,0].scatter(app_data['client_packets'], app_data['server_packets'], \n",
    "                     alpha=0.6, label=app)\n",
    "axes[1,0].set_xlabel('Client Packets')\n",
    "axes[1,0].set_ylabel('Server Packets')\n",
    "axes[1,0].set_title('Client vs Server Packet Distribution')\n",
    "axes[1,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 5. Server port distribution\n",
    "port_counts = df.groupby(['app_type', 'server_port']).size().unstack(fill_value=0)\n",
    "port_counts.plot(kind='bar', stacked=True, ax=axes[1,1])\n",
    "axes[1,1].set_title('Server Port Usage by Application')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].legend(title='Server Port', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 6. Correlation heatmap\n",
    "correlation_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1,2])\n",
    "axes[1,2].set_title('Feature Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Application profile analysis\n",
    "print(\"\\nApplication Profile Analysis:\")\n",
    "profile_stats = df.groupby('app_type').agg({\n",
    "    'avg_pkt_size': ['mean', 'std'],\n",
    "    'flow_duration_ms': ['mean', 'std'],\n",
    "    'throughput_kbps': ['mean', 'std'],\n",
    "    'server_port': 'first'\n",
    "}).round(2)\n",
    "\n",
    "profile_stats.columns = ['_'.join(col).strip() for col in profile_stats.columns]\n",
    "print(profile_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Data Preprocessing and Splitting\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Data Preprocessing and Splitting ---\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['app_type'])\n",
    "y = df['app_type']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Features: {list(X.columns)}\")\n",
    "\n",
    "# Encode the string labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(f\"\\nLabel encoding:\")\n",
    "for i, class_name in enumerate(le.classes_):\n",
    "    print(f\"  {class_name}: {i}\")\n",
    "\n",
    "# Use a stratified split to ensure all app types are represented\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Display class distribution in train/test sets\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "train_dist = pd.Series(y_train).value_counts().sort_index()\n",
    "for i, count in enumerate(train_dist):\n",
    "    print(f\"  {le.classes_[i]}: {count} samples\")\n",
    "\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "test_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "for i, count in enumerate(test_dist):\n",
    "    print(f\"  {le.classes_[i]}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Model Training\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Model Training ---\")\n",
    "\n",
    "# Initialize RandomForest with optimized parameters\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42, \n",
    "    n_jobs=-1,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2\n",
    ")\n",
    "\n",
    "print(\"Training the RandomForestClassifier...\")\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# Display model parameters\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"• Number of estimators: {model.n_estimators}\")\n",
    "print(f\"• Max depth: {model.max_depth}\")\n",
    "print(f\"• Min samples split: {model.min_samples_split}\")\n",
    "print(f\"• Min samples leaf: {model.min_samples_leaf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Model Evaluation\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Model Evaluation ---\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Per-class accuracy analysis\n",
    "print(\"\\nPer-Class Performance Analysis:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "for i, class_name in enumerate(le.classes_):\n",
    "    class_accuracy = cm[i, i] / np.sum(cm[i, :])\n",
    "    print(f\"  {class_name:12}: {class_accuracy:.3f} ({class_accuracy*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Results Visualization\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Results Visualization ---\")\n",
    "\n",
    "# Create comprehensive evaluation visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Container Application Classification Results', fontsize=16)\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='rocket', \n",
    "            xticklabels=le.classes_, yticklabels=le.classes_, ax=ax1)\n",
    "ax1.set_title('Confusion Matrix for Container Application Classification')\n",
    "ax1.set_ylabel('Actual Application')\n",
    "ax1.set_xlabel('Predicted Application')\n",
    "\n",
    "# 2. Feature Importance\n",
    "importances = model.feature_importances_\n",
    "features = X.columns\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': features, \n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "ax2.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "ax2.set_title('Feature Importance in Container Traffic Classification')\n",
    "ax2.set_xlabel('Importance')\n",
    "\n",
    "# 3. Prediction confidence distribution\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "max_probabilities = np.max(y_pred_proba, axis=1)\n",
    "ax3.hist(max_probabilities, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "ax3.set_title('Prediction Confidence Distribution')\n",
    "ax3.set_xlabel('Maximum Probability')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.axvline(np.mean(max_probabilities), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(max_probabilities):.3f}')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Application classification accuracy\n",
    "class_accuracies = [cm[i, i] / np.sum(cm[i, :]) for i in range(len(le.classes_))]\n",
    "ax4.bar(le.classes_, class_accuracies, color='skyblue', alpha=0.7)\n",
    "ax4.set_title('Per-Application Classification Accuracy')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_ylim(0, 1)\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add accuracy values on bars\n",
    "for i, acc in enumerate(class_accuracies):\n",
    "    ax4.text(i, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display feature importance table\n",
    "print(\"\\n--- Feature Importance: What defines a container's network personality? ---\")\n",
    "feature_importance_sorted = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "print(feature_importance_sorted.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "misclassification-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Misclassification Analysis\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Misclassification Analysis ---\")\n",
    "\n",
    "# Find misclassified samples\n",
    "misclassified_mask = (y_test != y_pred)\n",
    "misclassified_indices = np.where(misclassified_mask)[0]\n",
    "\n",
    "print(f\"Total misclassified samples: {len(misclassified_indices)} out of {len(y_test)} ({len(misclassified_indices)/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "if len(misclassified_indices) > 0:\n",
    "    # Analyze misclassification patterns\n",
    "    misclass_analysis = pd.DataFrame({\n",
    "        'Actual': [le.classes_[y_test[i]] for i in misclassified_indices],\n",
    "        'Predicted': [le.classes_[y_pred[i]] for i in misclassified_indices],\n",
    "        'Confidence': [max_probabilities[i] for i in misclassified_indices]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nMost common misclassification patterns:\")\n",
    "    misclass_patterns = misclass_analysis.groupby(['Actual', 'Predicted']).size().sort_values(ascending=False)\n",
    "    print(misclass_patterns.head(10))\n",
    "    \n",
    "    print(f\"\\nAverage confidence in misclassified predictions: {misclass_analysis['Confidence'].mean():.3f}\")\n",
    "    print(f\"Average confidence in correct predictions: {max_probabilities[~misclassified_mask].mean():.3f}\")\n",
    "    \n",
    "    # Show some examples of misclassified samples\n",
    "    print(\"\\nExamples of misclassified samples:\")\n",
    "    X_test_reset = X_test.reset_index(drop=True)\n",
    "    sample_indices = misclassified_indices[:5]  # Show first 5 misclassified samples\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        actual_app = le.classes_[y_test[idx]]\n",
    "        predicted_app = le.classes_[y_pred[idx]]\n",
    "        confidence = max_probabilities[idx]\n",
    "        print(f\"\\nSample {idx}:\")\n",
    "        print(f\"  Actual: {actual_app}, Predicted: {predicted_app} (confidence: {confidence:.3f})\")\n",
    "        print(f\"  Features: {dict(X_test_reset.iloc[idx].round(2))}\")\n",
    "\n",
    "# Calculate feature-based separation analysis\n",
    "print(\"\\n--- Feature Separation Analysis ---\")\n",
    "feature_stats = df.groupby('app_type')[['avg_pkt_size', 'flow_duration_ms', 'throughput_kbps']].agg(['mean', 'std'])\n",
    "print(\"\\nKey features by application type:\")\n",
    "print(feature_stats.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Application Prediction Examples\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Application Prediction Examples ---\")\n",
    "\n",
    "# Create example network flows for prediction\n",
    "example_flows = [\n",
    "    # WebApp: Small packets, HTTPS port, moderate duration\n",
    "    {'avg_pkt_size': 480, 'server_port': 443, 'flow_duration_ms': 450, \n",
    "     'client_packets': 15, 'server_packets': 8, 'total_bytes': 11040, 'throughput_kbps': 196},\n",
    "    \n",
    "    # Database: Large packets, PostgreSQL port, short duration\n",
    "    {'avg_pkt_size': 980, 'server_port': 5432, 'flow_duration_ms': 95, \n",
    "     'client_packets': 8, 'server_packets': 12, 'total_bytes': 19600, 'throughput_kbps': 1651},\n",
    "    \n",
    "    # Cache: Small packets, Redis port, very short duration\n",
    "    {'avg_pkt_size': 140, 'server_port': 6379, 'flow_duration_ms': 18, \n",
    "     'client_packets': 5, 'server_packets': 5, 'total_bytes': 1400, 'throughput_kbps': 622},\n",
    "    \n",
    "    # MessageQueue: Medium packets, RabbitMQ port, long duration\n",
    "    {'avg_pkt_size': 320, 'server_port': 5672, 'flow_duration_ms': 9500, \n",
    "     'client_packets': 12, 'server_packets': 13, 'total_bytes': 8000, 'throughput_kbps': 6.7},\n",
    "    \n",
    "    # APIGateway: Large packets, custom port, short duration\n",
    "    {'avg_pkt_size': 750, 'server_port': 8080, 'flow_duration_ms': 180, \n",
    "     'client_packets': 14, 'server_packets': 6, 'total_bytes': 15000, 'throughput_kbps': 667}\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "examples_df = pd.DataFrame(example_flows)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(examples_df)\n",
    "prediction_probabilities = model.predict_proba(examples_df)\n",
    "\n",
    "print(\"\\nContainer Application Predictions:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, (flow, pred_idx) in enumerate(zip(example_flows, predictions)):\n",
    "    predicted_app = le.classes_[pred_idx]\n",
    "    confidence = np.max(prediction_probabilities[i])\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Network Flow Characteristics:\")\n",
    "    print(f\"    • Average packet size: {flow['avg_pkt_size']} bytes\")\n",
    "    print(f\"    • Server port: {flow['server_port']}\")\n",
    "    print(f\"    • Flow duration: {flow['flow_duration_ms']} ms\")\n",
    "    print(f\"    • Client/Server packets: {flow['client_packets']}/{flow['server_packets']}\")\n",
    "    print(f\"    • Total bytes: {flow['total_bytes']:,}\")\n",
    "    print(f\"    • Throughput: {flow['throughput_kbps']:.1f} Kbps\")\n",
    "    \n",
    "    print(f\"  Prediction: {predicted_app} (confidence: {confidence:.3f})\")\n",
    "    \n",
    "    # Show probability distribution\n",
    "    print(f\"  Probability distribution:\")\n",
    "    for j, app_name in enumerate(le.classes_):\n",
    "        prob = prediction_probabilities[i][j]\n",
    "        print(f\"    {app_name:12}: {prob:.3f} {'█' * int(prob * 20)}\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "#  Conclusion\n",
    "# ==================================================================================\n",
    "\n",
    "print(\"--- Conclusion ---\")\n",
    "print(\"The RandomForest model successfully learned to classify containerized applications based on their distinct network traffic patterns.\")\n",
    "\n",
    "print(\"\\nKey Performance Results:\")\n",
    "print(f\"• Overall accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "print(f\"• Total samples analyzed: {len(df):,}\")\n",
    "print(f\"• Number of application types: {len(le.classes_)}\")\n",
    "print(f\"• Training time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"• Average prediction confidence: {np.mean(max_probabilities):.3f}\")\n",
    "\n",
    "print(\"\\nMost Important Network Characteristics:\")\n",
    "top_features = feature_importance_sorted.head(3)\n",
    "for idx, row in top_features.iterrows():\n",
    "    print(f\"• {row['Feature']}: {row['Importance']:.3f} importance\")\n",
    "\n",
    "print(\"\\nBusiness Impact:\")\n",
    "print(\"• **Container Visibility**: Automatically identify application types without deep packet inspection\")\n",
    "print(\"• **Resource Optimization**: Right-size containers based on application network behavior\")\n",
    "print(\"• **Security Monitoring**: Detect anomalous applications or configuration drift\")\n",
    "print(\"• **Network Planning**: Optimize network policies based on application traffic patterns\")\n",
    "\n",
    "print(\"\\nOperational Applications:\")\n",
    "print(\"• **Auto-Discovery**: Automatically classify unknown containers in production\")\n",
    "print(\"• **Performance Monitoring**: Baseline normal behavior for each application type\")\n",
    "print(\"• **Compliance**: Ensure containers match expected application profiles\")\n",
    "print(\"• **Anomaly Detection**: Flag containers with unexpected network behavior\")\n",
    "\n",
    "print(\"\\nTechnical Insights:\")\n",
    "print(\"• Server port is highly predictive but not sufficient alone\")\n",
    "print(\"• Packet size and flow duration create distinct 'network personalities'\")\n",
    "print(\"• Client/server packet ratios reveal application communication patterns\")\n",
    "print(\"• Throughput characteristics distinguish between high and low bandwidth applications\")\n",
    "\n",
    "print(\"\\nReal-world Deployment:\")\n",
    "print(\"• Integrate with container orchestration platforms (Kubernetes, Docker Swarm)\")\n",
    "print(\"• Deploy as sidecar containers for real-time classification\")\n",
    "print(\"• Use with network monitoring tools (Prometheus, Grafana) for dashboards\")\n",
    "print(\"• Combine with policy engines for automated container management\")\n",
    "\n",
    "print(f\"\\nModel Reliability:\")\n",
    "if len(misclassified_indices) > 0:\n",
    "    print(f\"• Misclassification rate: {len(misclassified_indices)/len(y_test)*100:.1f}%\")\n",
    "    print(f\"• Most confused applications require additional feature engineering\")\n",
    "else:\n",
    "    print(\"• Perfect classification on test set - model generalizes well\")\n",
    "print(f\"• High prediction confidence indicates robust decision boundaries\")\n",
    "print(f\"• Feature importance aligns with networking domain knowledge\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}