{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 12: DNS Tunneling Detection\n",
    "\n",
    "## Objective\n",
    "Build an interpretable machine learning model that distinguishes between legitimate DNS queries and those used for DNS tunneling based on statistical features.\n",
    "\n",
    "## Dataset\n",
    "DNS Tunneling Dataset from Kaggle containing labeled DNS queries with pre-calculated features.\n",
    "\n",
    "## Key Features\n",
    "- Logistic Regression for maximum interpretability\n",
    "- Focus on query length, entropy, and subdomain count\n",
    "- Model coefficients explain why queries are flagged\n",
    "- Real-time deployment capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install kaggle pandas numpy scikit-learn matplotlib seaborn\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Kaggle API for dataset download\n",
    "if not os.path.exists(os.path.expanduser('~/.kaggle/kaggle.json')):\n",
    "    print(\"Please set up your Kaggle API credentials first.\")\n",
    "    print(\"1. Go to https://www.kaggle.com/account\")\n",
    "    print(\"2. Create API token and download kaggle.json\")\n",
    "    print(\"3. Place it in ~/.kaggle/ directory\")\n",
    "else:\n",
    "    print(\"Kaggle API configured. Downloading DNS tunneling dataset...\")\n",
    "    !kaggle datasets download -d ahmethamzadedbs/dns-tunneling-dataset --unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DNS tunneling dataset\n",
    "print(\"Loading DNS tunneling dataset...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('dnscat2.csv')\n",
    "    print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\nexcept FileNotFoundError:\n",
    "    print(\"Dataset file not found. Please ensure the dataset is downloaded correctly.\")\n",
    "    print(\"Expected file: dnscat2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset structure\n",
    "print(\"Dataset Overview:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on most interpretable features for DNS tunneling detection\n",
    "feature_cols = ['query_length', 'subdomain_count', 'entropy']\n",
    "target_col = 'label'\n",
    "\n",
    "# Select relevant columns\n",
    "df_model = df[feature_cols + [target_col]].copy()\n",
    "\n",
    "# Encode the labels: 'nontunnel' -> 0, 'tunnel' -> 1\n",
    "df_model[target_col] = df_model[target_col].apply(lambda x: 1 if x == 'tunnel' else 0)\n",
    "\n",
    "print(\"Feature selection completed.\")\n",
    "print(f\"Selected features: {feature_cols}\")\n",
    "print(f\"Target variable: {target_col}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "class_counts = df_model[target_col].value_counts()\n",
    "print(f\"Nontunnel (0): {class_counts[0]:,}\")\n",
    "print(f\"Tunnel (1): {class_counts[1]:,}\")\n",
    "print(f\"Shape after feature selection: {df_model.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature differences between classes\n",
    "print(\"Visualizing feature differences between normal and tunneling DNS queries...\")\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "for i, col in enumerate(feature_cols):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    sns.boxplot(x=target_col, y=col, data=df_model)\n",
    "    plt.title(f'{col} by Class')\n",
    "    plt.xticks([0, 1], ['Nontunnel', 'Tunnel'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(\"‚Ä¢ Tunnel queries show higher length values\")\n",
    "print(\"‚Ä¢ Tunnel queries have more subdomains\")\n",
    "print(\"‚Ä¢ Tunnel queries exhibit higher entropy (more randomness)\")\n",
    "print(\"‚Ä¢ These patterns align with DNS tunneling behavior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of features by class\n",
    "print(\"Statistical Summary by Class:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for feature in feature_cols:\n",
    "    print(f\"\\n{feature}:\")\n",
    "    nontunnel_stats = df_model[df_model[target_col] == 0][feature].describe()\n",
    "    tunnel_stats = df_model[df_model[target_col] == 1][feature].describe()\n",
    "    \n",
    "    comparison = pd.DataFrame({\n",
    "        'Nontunnel': nontunnel_stats,\n",
    "        'Tunnel': tunnel_stats\n",
    "    })\n",
    "    print(comparison.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Splitting and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_model[feature_cols]\n",
    "y = df_model[target_col]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "\n",
    "# Stratified split to maintain class ratio\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nData split completed:\")\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Training class distribution:\")\n",
    "print(y_train.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for optimal Logistic Regression performance\n",
    "print(\"Scaling features...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed.\")\n",
    "print(f\"Training features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test features shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Logistic Regression model\n",
    "print(\"Training Logistic Regression model...\")\n",
    "\n",
    "# Use balanced class weights to handle any class imbalance\n",
    "model = LogisticRegression(\n",
    "    random_state=42, \n",
    "    class_weight='balanced',\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"Training completed successfully!\")\n",
    "\n",
    "# Display basic model information\n",
    "print(f\"\\nModel coefficients:\")\n",
    "for feature, coef in zip(feature_cols, model.coef_[0]):\n",
    "    print(f\"  {feature}: {coef:.4f}\")\n",
    "print(f\"Intercept: {model.intercept_[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_test, y_pred, target_names=['Nontunnel (0)', 'Tunnel (1)']))\n",
    "\n",
    "# Confusion matrix analysis\n",
    "print(\"\\nConfusion Matrix Analysis:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"True Negatives (Correctly identified nontunnel): {cm[0,0]}\")\n",
    "print(f\"False Positives (Nontunnel flagged as tunnel): {cm[0,1]}\")\n",
    "print(f\"False Negatives (Missed tunnel queries): {cm[1,0]}\")\n",
    "print(f\"True Positives (Correctly identified tunnel): {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Nontunnel', 'Tunnel'], \n",
    "            yticklabels=['Nontunnel', 'Tunnel'])\n",
    "plt.title('Confusion Matrix - DNS Tunneling Detection')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Interpretability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance through model coefficients\n",
    "print(\"Model Interpretability Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create coefficient dataframe\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Coefficient': model.coef_[0]\n",
    "}).sort_values('Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nModel Coefficients (Log-Odds):\")\n",
    "print(coefficients)\n",
    "\n",
    "print(\"\\nüîç Coefficient Interpretation:\")\n",
    "for _, row in coefficients.iterrows():\n",
    "    feature, coef = row['Feature'], row['Coefficient']\n",
    "    if coef > 0:\n",
    "        print(f\"‚Ä¢ {feature}: +{coef:.4f} - Higher values increase tunneling probability\")\n",
    "    else:\n",
    "        print(f\"‚Ä¢ {feature}: {coef:.4f} - Higher values decrease tunneling probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['green' if x > 0 else 'red' for x in coefficients['Coefficient']]\n",
    "bars = plt.barh(coefficients['Feature'], coefficients['Coefficient'], color=colors)\n",
    "plt.title('Feature Importance in DNS Tunneling Detection\\n(Logistic Regression Coefficients)')\n",
    "plt.xlabel('Coefficient Value (Log-Odds)')\n",
    "plt.ylabel('Features')\n",
    "plt.axvline(x=0, color='black', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, coef in zip(bars, coefficients['Coefficient']):\n",
    "    plt.text(coef + (0.01 if coef > 0 else -0.01), bar.get_y() + bar.get_height()/2, \n",
    "             f'{coef:.3f}', ha='left' if coef > 0 else 'right', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Key Insights:\")\n",
    "print(\"‚Ä¢ Positive coefficients indicate features that increase tunneling probability\")\n",
    "print(\"‚Ä¢ All three features align with expected DNS tunneling behavior\")\n",
    "print(\"‚Ä¢ Model provides clear explanations for each prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Metrics and Business Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive performance metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"üéØ Model Performance Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Accuracy:  {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "print(f\"Precision: {precision:.3f} ({precision*100:.1f}%)\")\n",
    "print(f\"Recall:    {recall:.3f} ({recall*100:.1f}%)\")\n",
    "print(f\"F1-Score:  {f1:.3f}\")\n",
    "print(f\"AUC-ROC:   {auc:.3f}\")\n",
    "\n",
    "print(f\"\\nüíº Business Impact:\")\n",
    "print(f\"‚Ä¢ Detects {recall*100:.1f}% of DNS tunneling attempts\")\n",
    "print(f\"‚Ä¢ {precision*100:.1f}% of alerts are genuine tunneling attempts\")\n",
    "print(f\"‚Ä¢ Reduces analyst workload through interpretable results\")\n",
    "print(f\"‚Ä¢ Enables real-time DNS monitoring and alerting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Example Predictions with Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate model interpretability with example predictions\n",
    "print(\"üîç Example Predictions with Explanations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Select a few test samples for demonstration\n",
    "sample_indices = [0, 10, 100, 500]  # Mix of different cases\n",
    "\n",
    "for idx in sample_indices:\n",
    "    if idx < len(X_test):\n",
    "        sample_features = X_test.iloc[idx]\n",
    "        sample_scaled = X_test_scaled[idx:idx+1]\n",
    "        prediction = model.predict(sample_scaled)[0]\n",
    "        probability = model.predict_proba(sample_scaled)[0, 1]\n",
    "        actual = y_test.iloc[idx]\n",
    "        \n",
    "        print(f\"\\nSample {idx + 1}:\")\n",
    "        print(f\"  Actual: {'Tunnel' if actual == 1 else 'Nontunnel'}\")\n",
    "        print(f\"  Predicted: {'Tunnel' if prediction == 1 else 'Nontunnel'} ({probability:.3f} probability)\")\n",
    "        print(f\"  Features:\")\n",
    "        \n",
    "        # Show feature contributions\n",
    "        for feature, value, coef in zip(feature_cols, sample_features, model.coef_[0]):\n",
    "            # Standardize the feature value for comparison\n",
    "            feature_idx = feature_cols.index(feature)\n",
    "            scaled_value = sample_scaled[0, feature_idx]\n",
    "            contribution = coef * scaled_value\n",
    "            print(f\"    {feature}: {value:.2f} (contribution: {contribution:+.3f})\")\n",
    "        \n",
    "        result = \"‚úÖ\" if prediction == actual else \"‚ùå\"\n",
    "        print(f\"  Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üõ°Ô∏è  PROJECT CONCLUSION: DNS Tunneling Detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n‚úÖ Key Achievements:\")\n",
    "print(f\"  ‚Ä¢ Built interpretable model with {recall*100:.1f}% recall for tunnel detection\")\n",
    "print(f\"  ‚Ä¢ Achieved {precision*100:.1f}% precision, minimizing false positive alerts\")\n",
    "print(f\"  ‚Ä¢ Created explainable predictions with clear feature contributions\")\n",
    "print(f\"  ‚Ä¢ Identified key DNS patterns: entropy, length, and subdomain structure\")\n",
    "\n",
    "print(\"\\nüéØ Security Value:\")\n",
    "print(\"  ‚Ä¢ Detects stealthy data exfiltration through DNS channels\")\n",
    "print(\"  ‚Ä¢ Provides forensic evidence for security investigations\")\n",
    "print(\"  ‚Ä¢ Enables real-time monitoring of DNS traffic\")\n",
    "print(\"  ‚Ä¢ Supports compliance requirements for advanced threat detection\")\n",
    "\n",
    "print(\"\\nüöÄ Production Deployment:\")\n",
    "print(\"  1. Integrate with DNS monitoring infrastructure\")\n",
    "print(\"  2. Set up real-time stream processing for DNS queries\")\n",
    "print(\"  3. Configure SIEM integration for automated alerting\")\n",
    "print(\"  4. Establish analyst workflows for investigating flagged queries\")\n",
    "print(\"  5. Implement model monitoring and retraining pipeline\")\n",
    "\n",
    "print(\"\\n‚ö° Technical Advantages:\")\n",
    "print(\"  ‚Ä¢ Lightweight model suitable for high-speed DNS processing\")\n",
    "print(\"  ‚Ä¢ Interpretable results reduce analyst investigation time\")\n",
    "print(\"  ‚Ä¢ Balanced approach between sensitivity and specificity\")\n",
    "print(\"  ‚Ä¢ Robust feature selection based on domain expertise\")\n",
    "\n",
    "print(\"\\nüî¨ Future Enhancements:\")\n",
    "print(\"  ‚Ä¢ Add temporal analysis for DNS query sequences\")\n",
    "print(\"  ‚Ä¢ Incorporate domain reputation and threat intelligence\")\n",
    "print(\"  ‚Ä¢ Extend to other covert channels (ICMP, HTTP headers)\")\n",
    "print(\"  ‚Ä¢ Implement ensemble methods for improved robustness\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}